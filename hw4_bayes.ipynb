{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifiers\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Naive Bayes is a class of simple classifiers based on Bayes' Rule and strong (or naive) independence assumptions between features. In this problem, you will implement a Naive Bayes Classifier for the Census Income Data Set from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/).\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "The dataset consists 32561 instances, each representing an individual. The goal is to predict whether a person makes over 50K a year based on 14 features. The features are:\n",
    "\n",
    "| column | type | description |\n",
    "| --- |:---:|:--- |\n",
    "| age | continuous | trips around the sun to date\n",
    "| final_weight | continuous | census weight attribute; constructed from the original census data |\n",
    "| education_num | continuous | numeric education scale -- their maximum educational level as a number |\n",
    "| capital_gain | continuous | income from investment sources |\n",
    "| capital_loss | continuous | losses from investment sources |\n",
    "| hours_per_week | continuous | number of hours worked every week |\n",
    "| work_class | categorical | `Private`, `Self-emp-not-inc`, `Self-emp-inc`, `Federal-gov`, `Local-gov`, `State-gov`, `Without-pay`, `Never-worked` |\n",
    "| education | categorical | `Bachelors`, `Some-college`, `11th`, `HS-grad`, `Prof-school`, `Assoc-acdm`, `Assoc-voc`, `9th`, `7th-8th`, `12th`, `Masters`, `1st-4th`, `10th`, `Doctorate`, `5th-6th`, `Preschool` |\n",
    "| marital_status | categorical | `Married-civ-spouse`, `Divorced`, `Never-married`, `Separated`, `Widowed`, `Married-spouse-absent`, `Married-AF-spouse` |\n",
    "| occupation | categorical | `Tech-support`, `Craft-repair`, `Other-service`, `Sales`, `Exec-managerial`, `Prof-specialty`, `Handlers-cleaners`, `Machine-op-inspct`, `Adm-clerical`, `Farming-fishing`, `Transport-moving`, `Priv-house-serv`, `Protective-serv`, `Armed-Forces` |\n",
    "| relationship | categorical | `Wife`, `Own-child`, `Husband`, `Not-in-family`, `Other-relative`, `Unmarried.` |\n",
    "| race | categorical | `White`, `Asian-Pac-Islander`, `Amer-Indian-Eskimo`, `Other`, `Black` |\n",
    "| sex | categorical | `Female`, `Male` |\n",
    "| native_country | categorical | (41 values not shown here) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "import gzip\n",
    "from testing.testing import test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Data Preparation\n",
    "\n",
    "First, you need to load in the above data, provided to you as a CSV file. As the data is from UCI repository, it is already quite clean. However, some instances contain missing `occupation`, `native_country` or `work_class` (represented as ? in the CSV file) and these have to be discarded from the training set. Also, replace the `income` column with `label`, which is 1 if `income` is `>50K` and 0 otherwise. Finally, ensure you reset the index so the row numbers are contiguous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "### TESTING load_data: PASSED 0/0\n###\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def read_csv(fn):\n",
    "    with gzip.open(fn, \"rt\", newline='', encoding=\"UTF-8\") as file:\n",
    "        return pd.read_csv(file)\n",
    "\n",
    "def load_data_test(load_data):\n",
    "    pass\n",
    "    # df = load_data()\n",
    "    # \n",
    "    # DF_TYPES = {\n",
    "    #     \"age\"  : \"int64\",\n",
    "    #     \"work_class\"  : \"object\",\n",
    "    #     \"final_weight\"  : \"int64\",\n",
    "    #     \"education\"  : \"object\",\n",
    "    #     \"education_num\"  : \"int64\",\n",
    "    #     \"marital_status\"  : \"object\",\n",
    "    #     \"occupation\"  : \"object\",\n",
    "    #     \"relationship\"  : \"object\",\n",
    "    #     \"race\"  : \"object\",\n",
    "    #     \"sex\"  : \"object\",\n",
    "    #     \"capital_gain\"  : \"int64\",\n",
    "    #     \"capital_loss\"  : \"int64\",\n",
    "    #     \"hours_per_week\"  : \"int64\",\n",
    "    #     \"native_country\"  : \"object\",\n",
    "    #     \"label\"  : \"int64\"\n",
    "    # }\n",
    "    # \n",
    "    # test.equal(DF_TYPES, { k: str(df[k].dtypes) for k in DF_TYPES })\n",
    "    # \n",
    "    # # Check for blank entries:\n",
    "    # test.equal(any(df['occupation'].eq(\"?\")), False)\n",
    "    # test.equal(any(df['native_country'].eq(\"?\")), False)\n",
    "    # test.equal(any(df['work_class'].eq(\"?\")), False)\n",
    "    # \n",
    "    # # Make sure there's no income column:\n",
    "    # test.true(\"income\" not in df.columns)\n",
    "    # \n",
    "    # # Index handling:\n",
    "    # test.equal(repr(df.index), \"RangeIndex(start=0, stop=30162, step=1)\")\n",
    "    \n",
    "@test\n",
    "def load_data(file_name=\"census.csv.gz\"):\n",
    "    \"\"\" loads and processes data in the manner specified above\n",
    "\n",
    "    args:\n",
    "        file_name : str -- path to csv file containing data\n",
    "\n",
    "    returns: pd.DataFrame -- processed dataframe\n",
    "    \"\"\"\n",
    "    df = read_csv(file_name)\n",
    "    df = df.replace(\"?\",np.nan)\n",
    "    df = df.dropna(axis=0)\n",
    "    df = df.replace(\">50K\",1)\n",
    "    df = df.replace(\"<=50K\",0)\n",
    "    df = df.rename(columns={\"income\":   \"label\"})\n",
    "    df = df.reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Naive Bayes classifier\n",
    "\n",
    "Let $X_1, X_2, \\ldots, X_k$ be the $k$ features of a dataset, with class label given by the variable $y$. A probabilistic classifier assigns the most probable class to each instance $(x_1,\\ldots,x_k)$, as expressed by\n",
    "$$ \\hat{y} = \\arg\\max_y P(y\\ \\mid\\ x_1,\\ldots,x_k) $$\n",
    "\n",
    "Using Bayes' theorem, the above *posterior probability* can be rewritten as\n",
    "$$ P(y\\ \\mid\\ x_1,\\ldots,x_k) = \\frac{P(y) P(x_1,\\ldots,x_n\\ \\mid\\ y)}{P(x_1,\\ldots,x_k)} $$\n",
    "where\n",
    "- $P(y)$ is the prior probability of the class\n",
    "- $P(x_1,\\ldots,x_k\\ \\mid\\ y)$ is the likelihood of data under a class\n",
    "- $P(x_1,\\ldots,x_k)$ is the evidence for data\n",
    "\n",
    "Naive Bayes classifiers assume that the feature values are conditionally independent given the class label, that is,\n",
    "$ P(x_1,\\ldots,x_n\\ \\mid\\ y) = \\prod_{i=1}^{k}P(x_i\\ \\mid\\ y) $. This strong assumption helps simplify the expression for posterior probability to\n",
    "$$ P(y\\ \\mid\\ x_1,\\ldots,x_k) = \\frac{P(y) \\prod_{i=1}^{k}P(x_i\\ \\mid\\ y)}{P(x_1,\\ldots,x_k)} $$\n",
    "\n",
    "For a given input $(x_1,\\ldots,x_k)$, $P(x_1,\\ldots,x_k)$ is constant. Hence, we can say that:\n",
    "$$ P(y\\ \\mid\\ x_1,\\ldots,x_k) \\propto P(y) \\prod_{i=1}^{k}P(x_i\\ \\mid\\ y) $$\n",
    "\n",
    "Thus, the class of a new instance can be predicted as:\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{k}P(x_i\\ \\mid\\ y)$$\n",
    "\n",
    "where $P(y)$ is commonly known as the **class prior** and $P(x_i\\ \\mid\\ y)$ is the **feature predictor**.\n",
    "\n",
    "Observe that this is the product of $k+1$ probability values, which can result in very small numbers. When working with real-world data, this often leads to an [arithmetic underflow](https://en.wikipedia.org/wiki/Arithmetic_underflow). We will instead be adding the logarithm of the probabilities:\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_y \\underbrace{\\log P(y)}_\\text{log-prior} + \\underbrace{\\sum_{i=1}^{k} \\log P(x_i\\ \\mid\\ y)}_\\text{log-likelihood}$$\n",
    "\n",
    "The rest of the assignment deals with how each of these probability distributions -- $P(y), P(x_1\\ \\mid\\ y), \\ldots, P(x_k\\ \\mid\\ y)$ -- are estimated from data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Predictor\n",
    "\n",
    "Naive Bayes classifiers are popular because we can independently model each feature and mix-and-match model types based on the prior knowledge. For example, we might know (or assume) that $(X_i|y)$ has some distribution, so we can directly use the probability density or mass function of the distribution to model $(X_i|y)$.\n",
    "\n",
    "In this assignment, you will be using two classes of likelihood models:\n",
    "- Gaussian models, for continuous real-valued features (parameterized by mean $\\mu$ and variance $\\sigma$)\n",
    "- Categorical models, for features in discrete categories (parameterized by $\\mathbf{p} = <p_0,p_1\\ldots>$, one parameter per category)\n",
    "\n",
    "You need to implement a generic predictor class for each type of model. Your class should have the following methods:\n",
    "\n",
    "- `fit()`: Learn parameters for the likelihood model using an appropriate Maximum Likelihood Estimator.\n",
    "- `partial_log_likelihood()`: Use the previously learnt parameters to compute the probability density or mass of a given feature value, and return the natural logarithm of this value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Gaussian Feature Predictor\n",
    "\n",
    "The Gaussian distribution is characterized by two parameters - mean $\\mu$ and standard deviation $\\sigma$:\n",
    "$$ f_Z(z) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp{(-\\frac{(z-\\mu)^2}{2\\sigma^2})} $$\n",
    "\n",
    "Given $n$ samples $z_1, \\ldots, z_n$ from the above distribution, the MLE for mean and standard deviation are:\n",
    "$$ \\hat{\\mu} = \\frac{1}{n} \\sum_{j=1}^{n} z_j $$\n",
    "\n",
    "$$ \\hat{\\sigma} = \\sqrt{\\frac{1}{n} \\sum_{j=1}^{n} (z_j-\\hat{\\mu})^2} $$\n",
    "\n",
    "`scipy.stats.norm` may be helpful, as may `pandas.DataFrame.var`. If you use the latter, remember to correctly set the `ddof`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "### TESTING gaussian_pred: PASSED 0/0\n###\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def gaussian_pred_test(gaussian_predictor):\n",
    "    pass\n",
    "    # g = gaussian_predictor(2)\n",
    "    # \n",
    "    # np.random.seed(0xDEADBEEF)\n",
    "    # rnd = np.random.normal(loc=0.0, scale=1.0, size=(1000,))\n",
    "    # \n",
    "    # data = pd.Series(np.concatenate([rnd, 100-rnd]))\n",
    "    # labels = pd.Series(np.array([0]*1000 + [1]*1000))\n",
    "    # \n",
    "    # g.fit(data, labels)\n",
    "    # \n",
    "    # test.equal(tuple(g.partial_log_likelihood([0., 50., 100.]).shape), (2, 3))\n",
    "    # # If the equality is not exact, you may need to change the test to ensure the absolute difference is no more than 1e-4\n",
    "    # test.true(np.allclose(g.partial_log_likelihood([0., 50., 100.]), [[-0.9234573135702573, -1242.233086628376, -4963.217354198167], [-4963.217354198166, -1242.2330866283753, -0.9234573135702564]], rtol=0, atol=1e-4))\n",
    "\n",
    "class GaussianPredictor:\n",
    "    \n",
    "    # use logpdf? one value per x\n",
    "    # two for loops\n",
    "    # 0, 50, 100\n",
    "    # for i in range\n",
    "    # X[ y == i for i in ...]\n",
    "    # series\n",
    "    \"\"\" Feature predictor for a normally distributed real-valued, continuous feature.\n",
    "\n",
    "        attr:\n",
    "            k : int -- number of classes\n",
    "            mu : np.ndarray[k] -- vector containing per class mean of the feature\n",
    "            sigma : np.ndarray[k] -- vector containing per class std. deviation of the feature\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k):\n",
    "        \"\"\" constructor\n",
    "\n",
    "        args : k -- number of classes\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.mu = np.zeros(k)\n",
    "        self.sigma = np.zeros(k)\n",
    "        pass\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        \"\"\"update predictor statistics (mu, sigma) for Gaussian distribution\n",
    "\n",
    "        args:\n",
    "            x : pd.Series -- feature values\n",
    "            y : np.Series -- class labels\n",
    "            \n",
    "        return : GaussianPredictor -- return self for convenience\n",
    "        \n",
    "        \"\"\"\n",
    "        # df = pd.DataFrame({\"values\":x,\"labels\":y})\n",
    "        # groups = df.groupby(\"labels\")\n",
    "        # self.mu = np.array((groups.mean()))\n",
    "        # self.sigma = np.sqrt(np.array(groups.var(ddof=0)))\n",
    "        y=np.array(y)\n",
    "        x=np.array(x)\n",
    "        \n",
    "        \n",
    "        for i in range(self.k):\n",
    "            ybools = (y==i)\n",
    "            # given_i = np.array([x[j] for j in range(len(y)) if y[j] == i])\n",
    "            given_i = np.extract(ybools,x)\n",
    "            self.mu[i]=given_i.mean()\n",
    "            self.sigma[i]=np.sqrt(given_i.var())\n",
    "        return self\n",
    "            \n",
    "    def partial_log_likelihood(self, x):\n",
    "        \"\"\" log likelihood of feature values x according to each class\n",
    "\n",
    "        args:\n",
    "            x : pd.Series -- feature values\n",
    "\n",
    "        return: np.ndarray[self.k, len(x)] : log likelihood for this feature for each class\n",
    "        \"\"\"\n",
    "        logpdfs = np.zeros((self.k,len(x)))\n",
    "        # for i in range(self.k):\n",
    "        #     for j in range(len(x)):\n",
    "        #         logpdfs[i][j]=stats.norm(loc=self.mu[i],scale=self.sigma[i]).logpdf(x[j])\n",
    "        for i in range(self.k):\n",
    "            logpdfs[i]=stats.norm(loc=self.mu[i],scale=self.sigma[i]).logpdf(x)\n",
    "        return logpdfs\n",
    "@test\n",
    "def gaussian_pred(k):\n",
    "    return GaussianPredictor(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Categorical Feature Predictor\n",
    "\n",
    "The categorical distribution with $l$ categories $\\{0,\\ldots,l-1\\}$ is characterized by parameters $\\mathbf{p} = (p_0,\\dots,p_{l-1})$ where $\\sum\\mathbf p = 1$.\n",
    "\n",
    "If $C$ is categorically distributed, the probability of observing $z$ is:\n",
    "\n",
    "$$ \\Pr(C=z; \\mathbf{p}) = \\begin{cases}\n",
    "    p_0 & \\text{ if } z=0\n",
    "\\\\  p_1 & \\text{ if } z=1\n",
    "\\\\  \\vdots\n",
    "\\\\  p_{l-1} & \\text{ if } z=(l-1)\n",
    "\\end{cases}$$\n",
    "\n",
    "Given $n$ samples $z_1, \\ldots, z_n$ from $C$, the smoothed Maximum Likelihood Estimator for $\\mathbf p$ is:\n",
    "$$ \\hat{p_t} = \\frac{n_t + \\alpha}{n + l\\alpha} $$\n",
    "\n",
    "where $n_t = \\sum_{j=1}^{n} [z_j=t]$ (i.e., the number of times the label $t$ occurred in the sample) and $n$ is the total number of samples. The smoothing is done to avoid zero-count problem (similar in spirit to $n$-gram model in NLP.)\n",
    "\n",
    "In this problem, you need to write a predictor that learns a different categorical distribution $C_i$ for each of $k$ possible classes. You should maintain a dictionary from each possible input token (i.e. each value) to an array of length $k$ that contains $(\\Pr(C_0=z), \\Pr(C_1=z), ..., \\Pr(C_{k-1}=z))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "### TESTING categorical_pred: PASSED 0/0\n###\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "#ln (100/102) \n",
    "#ln (ln 1/102)\n",
    "# 3 is the number of classes\n",
    "def categorical_pred_test(categorical_pred):\n",
    "    # d = categorical_pred(3)\n",
    "    # \n",
    "    # data = pd.Series([\"A\"]*99 + [\"B\"]*99 + [\"C\"]*99)\n",
    "    # labels = pd.Series([0]*99 + [1]*99 + [2]*99)\n",
    "    # d.fit(data, labels)\n",
    "    # \n",
    "    # pll = d.partial_log_likelihood([\"A\", \"B\", \"C\", \"A\", \"B\", \"C\"])\n",
    "    # test.equal(tuple(pll.shape), (3, 6))\n",
    "    # n = -4.624972813284271\n",
    "    # p = -0.019802627296179754\n",
    "    # test.equal(pll.tolist(), [[p, n, n, p, n, n], [n, p, n, n, p, n], [n, n, p, n, n, p]])\n",
    "    # \n",
    "    # p = categorical_pred(3)\n",
    "    # \n",
    "    # data = pd.Series([\"A\"]*99 + [\"B\"]*99 + [\"C\"]*99)\n",
    "    # labels = pd.Series([0]*99 + [1]*99 + [2]*99)\n",
    "    # p.fit(data, labels)\n",
    "    # \n",
    "    # test.true(np.allclose(p.p['A'], [0.98039216, 0.00980392, 0.00980392], atol=1e-6))\n",
    "    # \n",
    "    # pll = p.partial_log_likelihood([\"A\", \"B\", \"C\", \"A\", \"B\", \"C\"])\n",
    "    # test.equal(tuple(pll.shape), (3, 6))\n",
    "    # n = np.log(1/102)\n",
    "    # p = np.log(100/102)\n",
    "    # \n",
    "    # test.true(np.allclose(pll, [[p, n, n, p, n, n], [n, p, n, n, p, n], [n, n, p, n, n, p]]))\n",
    "    pass\n",
    "    # p = categorical_pred(2)\n",
    "    # \n",
    "    # \n",
    "    # data = pd.Series([\"A\"]*50 + [\"B\"]*50 + [\"C\"]*50)\n",
    "    # labels = pd.Series([0]*75 + [1]*75)\n",
    "    # p.fit(data, labels)\n",
    "    # # print(p.p)\n",
    "    # \n",
    "    # \n",
    "    # test.true(np.allclose(p.p['A'], [0.65384614, 0.01282051], atol=1e-6))\n",
    "    # test.true(np.allclose(p.p['B'], [0.33333334, 0.33333334], atol=1e-6))\n",
    "    # test.true(np.allclose(p.p['C'], [0.01282051, 0.65384614], atol=1e-6))\n",
    "    # \n",
    "    # \n",
    "    # \n",
    "    # pll = p.partial_log_likelihood([\"A\", \"B\", \"C\"])\n",
    "    # test.equal(tuple(pll.shape), (2, 3))\n",
    "    # n = np.log(1/78)\n",
    "    # m = np.log(51/78)\n",
    "    # l = np.log(26/78)\n",
    "    # \n",
    "    # \n",
    "    # \n",
    "    # test.true(np.allclose(pll, [[m, l, n], [n, l, m]], atol=1e-6))\n",
    "    \n",
    "\n",
    "class CategoricalPredictor:\n",
    "    \"\"\" Feature predictor for a categorical feature.\n",
    "\n",
    "        attr: \n",
    "            k : int -- number of classes\n",
    "            p : Dict[feature_value, np.ndarray[k]] -- dictionary of vectors containing per-class probability of a feature value;\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k):\n",
    "        \"\"\" constructor\n",
    "\n",
    "        args : k -- number of classes\n",
    "        \"\"\"\n",
    "        self.k=k\n",
    "        pass\n",
    "\n",
    "    def fit(self, x, y, alpha=1.):\n",
    "        \"\"\" initializes the predictor statistics (p) for Categorical distribution\n",
    "        \n",
    "        args:\n",
    "            x : pd.Series -- feature values\n",
    "            y : pd.Series -- class labels\n",
    "        \n",
    "        kwargs:\n",
    "            alpha : float -- smoothing factor\n",
    "\n",
    "        return : CategoricalPredictor -- returns self for convenience:\n",
    "        \"\"\"\n",
    "        y=np.array(y)\n",
    "        x=np.array(x)\n",
    "        self.p = {}\n",
    "        for char in set(x):\n",
    "            self.p[char]=np.zeros(self.k)\n",
    "            \n",
    "        \n",
    "        for i in range(self.k):\n",
    "            # n=sum([1 for j in range(len(y)) if y[j] == i])\n",
    "            ybools= (y==i)\n",
    "            \n",
    "            n=np.sum( ybools )\n",
    "            for char in set(x):\n",
    "                xbools= (x == char)\n",
    "                # nj=sum([1 for j in range(len(y)) if x[j] == char and y[j] == i ])\n",
    "                nj = np.sum(np.logical_and(xbools,ybools))\n",
    "                (self.p[char])[i] = (nj + alpha)/ (n+len(set(x))*alpha)\n",
    "        return self\n",
    "\n",
    "    def partial_log_likelihood(self, x):\n",
    "        \"\"\" log likelihood of feature values x according to each class\n",
    "\n",
    "        args:\n",
    "            x : pd.Series -- vector of feature values\n",
    "\n",
    "        return : np.ndarray[self.k, len(x)] -- matrix of log likelihood for this feature\n",
    "        \"\"\"\n",
    "        like = np.zeros((self.k,len(x)))\n",
    "        \n",
    "        for i in range(self.k):\n",
    "            for j,char in enumerate(x):\n",
    "                # try:\n",
    "                like[i][j]=np.log(self.p[char][i])\n",
    "                # except KeyError:\n",
    "                #     print(j,x[:10],self.p)\n",
    "        return like\n",
    "\n",
    "@test\n",
    "def categorical_pred(k):\n",
    "    return CategoricalPredictor(k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "   TIME_OF_DAY ILLUMINATION  WEATHER ROAD_CONDITION INTERSECT_TYPE  \\\n29          11            1        1              5              0   \n48          13            1        4              2              0   \n52          10            1        2              2              0   \n53          15            1        1              2              0   \n54          16            1        1              3              0   \n59          13            1        2              2              0   \n61          11            1        4              5              0   \n67          21            2        4              4              2   \n69           7            1        4              5              0   \n70          10            1        4              3              0   \n\n    SCH_BUS_IND  PERSON_COUNT  AUTOMOBILE_COUNT  MOTORCYCLE_COUNT  \\\n29            0             2                 2                 0   \n48            0             3                 2                 0   \n52            0             4                 2                 0   \n53            0             3                 2                 0   \n54            0             1                 1                 0   \n59            0             2                 2                 0   \n61            0             3                 1                 0   \n67            0             1                 1                 0   \n69            0             2                 2                 0   \n70            0             2                 1                 0   \n\n    HEAVY_TRUCK_COUNT  FATAL_COUNT  INJURY_COUNT  MAX_SEVERITY_LEVEL  \\\n29                  0            0             0                   0   \n48                  0            0             0                   0   \n52                  0            0             0                   0   \n53                  0            0             2                   4   \n54                  0            0             1                   4   \n59                  0            0             1                   4   \n61                  0            0             0                   0   \n67                  0            0             0                   0   \n69                  0            0             2                   3   \n70                  0            0             0                   0   \n\n   LANE_CLOSED  HAZARDOUS_TRUCK  MAJOR_INJURY SPEED_LIMIT  \n29           0                0             0          40  \n48           1                0             0          35  \n52           0                0             0          40  \n53           2                0             0          25  \n54           2                0             0          45  \n59           2                0             0          40  \n61           2                0             0          35  \n67           0                0             0          45  \n69           1                0             0          35  \n70           0                0             0          35  \n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 43610 entries, 1 to 180963\nData columns (total 7 columns):\nROAD_CONDITION    43610 non-null object\nINTERSECT_TYPE    43610 non-null object\nLANE_CLOSED       43610 non-null object\nTIME_OF_DAY       43610 non-null object\nSPEED_LIMIT       43610 non-null object\nILLUMINATION      43610 non-null object\nlabel             43610 non-null int64\ndtypes: int64(1), object(6)\nmemory usage: 2.7+ MB\nNone\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import wget\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "#clean sch_bus_ind y/n to integer 0,1\n",
    "\n",
    "def type_boolean(c):\n",
    "    if c == \"Y\": return 1\n",
    "    elif c == \"N\": return 0\n",
    "    # elif c == \"nan\": return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "    # raise ValueError(c)\n",
    "\n",
    "def ROAD_CONDITION(c): # 8 is other 9 is unknown, 1,7->2, 3->4, 4->3, 5,6->5, 2,8,9->nan\n",
    "    if c == 1 or c == 7:\n",
    "        return 2\n",
    "    elif c == 3:\n",
    "        return 4\n",
    "    elif c == 4:\n",
    "        return 3\n",
    "    elif c == 5 or c == 6:\n",
    "        return 5\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def INTERSECT_TYPE(c): # 10 is other 99 is unkonw\n",
    "    if c <= 9:\n",
    "        return c\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def ILLUMINATION(c):\n",
    "    if c <= 6:\n",
    "        return c\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def WEATHER(c):\n",
    "    if c <= 7:\n",
    "        return c\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def TIME(c): # extract only the hour\n",
    "    if c <= 2500:\n",
    "        return c // 100\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "if not Path('all-crashes-2004-2018.csv.zip').exists():\n",
    "    wget.download(\"https://data.wprdc.org/dataset/3130f583-9499-472b-bb5a-f63a6ff6059a/resource/ec578660-2d3f-489d-9ba1-af0ebfc3b140/download/all-crashes-2004-2018.csv.zip\")\n",
    "# zf = zipfile.ZipFile('all-crashes-2004-2018.csv.zip') \n",
    "df_io = pd.read_csv('all-crashes-2004-2018.csv.zip')\n",
    "# print(df.head())\n",
    "# print(list(df))\n",
    "# static_columns = \"ROAD_CONDITION,INTERSECT_TYPE,URBAN_RURAL,DISTRICT,STATE_ROAD,LOCAL_ROAD,SNOW_SLUSH_ROAD,LANE_CLOSED,TIME_OF_DAY,SPEED_LIMIT\"\n",
    "# dynamic_columns = \"ILLUMINATION,MOTORCYCLE_COUNT,HEAVY_TRUCK_COUNT,WEATHER,HAZARDOUS_TRUCK,SCH_BUS_IND,AUTOMOBILE_COUNT\"\n",
    "# output_columns = \"PERSON_COUNT,FATAL_COUNT,INJURY_COUNT,MAX_SEVERITY_LEVEL,MAJOR_INJURY\"\n",
    "# df_io = df[(static_columns+\",\"+dynamic_columns+\",\"+output_columns).split(',')]\n",
    "# print(df_io.head())\n",
    "# print(df_io.dtypes)\n",
    "# print(df_io.info())\n",
    "\n",
    "# df_io['SCH_BUS_IND'] = df_io['SCH_BUS_IND'].apply(type_boolean)\n",
    "\n",
    "\n",
    "\n",
    "# df_io['ROAD_CONDITION'] = df_io['ROAD_CONDITION'].apply(ROAD_CONDITION)\n",
    "# df_io['INTERSECT_TYPE'] = df_io['INTERSECT_TYPE'].apply(INTERSECT_TYPE)\n",
    "# df_io['ILLUMINATION'] = df_io['ILLUMINATION'].apply(ILLUMINATION)\n",
    "# df_io['WEATHER'] = df_io['WEATHER'].apply(WEATHER)\n",
    "# df_io['TIME_OF_DAY'] = df_io['TIME_OF_DAY'].apply(TIME)\n",
    "\n",
    "# df_io = df_io.astype(\"Int64\")\n",
    "\n",
    "# print(df_io.head())\n",
    "# print(df_io.dtypes)\n",
    "# print(df_io.info())\n",
    "\n",
    "# drop col that will not be used\n",
    "static = ['ROAD_CONDITION', 'INTERSECT_TYPE', 'LANE_CLOSED', 'TIME_OF_DAY', 'SPEED_LIMIT', 'ILLUMINATION']\n",
    "dynamic = ['MOTORCYCLE_COUNT', 'HEAVY_TRUCK_COUNT', 'HAZARDOUS_TRUCK', 'AUTOMOBILE_COUNT', 'SCH_BUS_IND', 'WEATHER']\n",
    "label = ['PERSON_COUNT', 'FATAL_COUNT', 'INJURY_COUNT', 'MAX_SEVERITY_LEVEL', 'MAJOR_INJURY']\n",
    "categorical = ['ROAD_CONDITION', 'INTERSECT_TYPE', 'LANE_CLOSED', 'ILLUMINATION', 'HAZARDOUS_TRUCK', 'SCH_BUS_IND', 'WEATHER']\n",
    "gussian = ['TIME_OF_DAY', 'SPEED_LIMIT', 'MOTORCYCLE_COUNT', 'HEAVY_TRUCK_COUNT', 'AUTOMOBILE_COUNT']\n",
    "data = static + dynamic\n",
    "for col in df_io.columns:\n",
    "    if col not in static and col not in dynamic and col not in label:\n",
    "        df_io.drop(col, axis = 1, inplace = True)  \n",
    "# print(df_io[15:25])\n",
    "# df_io['TIME_OF_DAY'] = df_io['TIME_OF_DAY'].astype(\"Int64\")\n",
    "\n",
    "\n",
    "# clean data\n",
    "df_io['SCH_BUS_IND'] = df_io['SCH_BUS_IND'].apply(type_boolean)\n",
    "df_io['ROAD_CONDITION'] = df_io['ROAD_CONDITION'].apply(ROAD_CONDITION)\n",
    "df_io['INTERSECT_TYPE'] = df_io['INTERSECT_TYPE'].apply(INTERSECT_TYPE)\n",
    "df_io['ILLUMINATION'] = df_io['ILLUMINATION'].apply(ILLUMINATION)\n",
    "df_io['WEATHER'] = df_io['WEATHER'].apply(WEATHER)\n",
    "df_io['TIME_OF_DAY'] = df_io['TIME_OF_DAY'].apply(TIME)\n",
    "\n",
    "# drop rows contain nan\n",
    "df_io = df_io.dropna()\n",
    "df_io = df_io.astype(\"int64\")\n",
    "# df_io[categorical] = df_io[categorical].astype(\"object\")\n",
    "# df_io = df_io.astype(\"object\")\n",
    "df_io[static] = df_io[static].astype(\"object\")\n",
    "print(df_io[15:25])\n",
    "# group data into dataset/label\n",
    "df_data = df_io[data].copy()\n",
    "df_label = df_io[label].copy()\n",
    "# print(df_data.info())\n",
    "# print(df_label.info())\n",
    "\n",
    "static_df_data = df_io[static+['MAX_SEVERITY_LEVEL']].copy()\n",
    "static_df_data = static_df_data.rename(columns={'MAX_SEVERITY_LEVEL':\"label\"})\n",
    "print(static_df_data.info())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Putting things together\n",
    "\n",
    "It's time to put all the feature predictors together and do something useful! You will implement a class that puts these classifiers to good use:\n",
    "\n",
    "- `__init__()`: Compute the log prior for each class and initialize the feature predictors (based on feature type). The smoothed prior for class $t$ is given by\n",
    "$$ \\text{prior}(t) = \\frac{n_t + \\alpha}{n + k\\alpha} $$\n",
    "where $n_t = \\sum_{j=1}^{n} [y_j=t]$, (i.e., the number of times the label $t$ occurred in the sample), $n$ is the number fo entries in the sample, and $k$ is the number of label values. \n",
    "- `log_likelihood()`: Compute the sum of the log prior and partial log likelihoods for all features. Use it to predict the final class label.\n",
    "- `predict()`: Use the output of log_likelihood to predict a class label; break ties by predicting the class with lower id.\n",
    "\n",
    "**Note:** Your implementation should not assume the data will always be the same as the census data. We may pass any dataset to your class. You can assume that:\n",
    "\n",
    "1. the input will contain a `label` column of type `int64` with values $0,\\ldots,k-1$ for some $k$\n",
    "2. all other columns will be either of type `object` (for categorical data) or `int64` (for integer data)\n",
    "3. if you encounter a column of an invalid type, throw an exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "ROAD_CONDITION\nCounter({-0.4625732344111501: 29495, -1.714689874031898: 6913, -1.9608807955276653: 5258, -3.0040717871794476: 1944})\nCounter({-0.22716231090912753: 29495, -2.286669637688146: 6913, -2.928523523860541: 5258, -3.0338840395183673: 1944})\nCounter({-0.2508343514593957: 29495, -2.1838687861546635: 6913, -2.6996819514316934: 5258, -3.169685580677429: 1944})\nCounter({-0.289554824465291: 29495, -2.108515970850141: 6913, -2.384105997536893: 5258, -3.275078921426758: 1944})\nCounter({-0.3172211412915822: 29495, -2.0214612325030807: 6913, -2.2937441709691435: 5258, -3.257603319762408: 1944})\nCounter({-1.3862943611198906: 43610})\nCounter({-1.3862943611198906: 43610})\nCounter({-1.3862943611198906: 43610})\nCounter({-0.3075069923984463: 29495, -1.980305073843198: 6913, -2.434022132975402: 5258, -3.2438935878434885: 1944})\nCounter({-0.32119406432510683: 29495, -2.0406555166446796: 6913, -2.205735267004128: 5258, -3.3637709761430385: 1944})\n",
      "INTERSECT_TYPE\nCounter({-0.42606658920162344: 27487, -1.8336504109372151: 7276, -1.9827632967345163: 6660, -3.8670582000600597: 940, -4.470926796181479: 506, -4.658243011949525: 384, -5.153454407913964: 280, -6.612773904048745: 54, -7.912056888179006: 18, -9.41613428495528: 5})\nCounter({-0.2790835671965491: 27487, -2.372318431008721: 13936, -4.56954300834494: 940, -5.262690188904886: 863, -3.876395827784995: 384})\nCounter({-0.45579555940850663: 27487, -1.7934247485471162: 7276, -1.955943678044891: 6660, -3.833645577073671: 940, -4.319153392855371: 506, -4.789157022101107: 384, -4.606835465307152: 280, -6.398594934535208: 77})\nCounter({-0.4873157008646667: 27487, -1.7521531942270117: 7276, -1.8244313397036576: 6660, -3.785074720271955: 940, -4.602519617509477: 506, -4.632372580659158: 384, -5.0843577044022155: 280, -6.519442229691538: 54, -7.4357329615656935: 18, -8.128880142125638: 5})\nCounter({-0.4903723521640719: 27487, -1.775864770984098: 7276, -1.7942273875586368: 6660, -3.9199418787819296: 940, -4.273958425054052: 506, -4.738666366725077: 384, -4.874211858721301: 280, -6.603450970968022: 54, -8.308199063206446: 23})\nCounter({-2.3025850929940455: 43610})\nCounter({-2.3025850929940455: 43610})\nCounter({-2.3025850929940455: 43610})\nCounter({-0.5944519909133963: 27487, -1.6606433123340436: 7276, -1.5981821427103073: 6660, -3.622301818357496: 940, -4.549063850098946: 506, -4.903235663819561: 384, -4.807925484015236: 280, -6.646204968878183: 54, -6.982677205499396: 18, -7.205820756813607: 5})\nCounter({-0.4629958920883308: 27487, -1.7303905228517629: 7276, -1.9868200517994394: 6660, -3.676300671907076: 940, -4.315380631196746: 506, -5.6503816979290855: 402, -4.861924337564815: 280, -6.161207321695077: 54, -7.259819610363186: 5})\n",
      "LANE_CLOSED\nCounter({-0.5231412857710867: 23675, -1.3181594812251252: 11996, -1.969596713574681: 7928, -8.49959932301519: 11})\nCounter({-1.9352717508502575: 23675, -2.286669637688146: 11996, -0.28946619424528236: 7928, -5.231108616854587: 11})\nCounter({-1.1956045546554195: 23675, -1.5522794985941517: 11996, -0.7256009254096838: 7928, -6.38856140554563: 11})\nCounter({-0.8450355272529108: 23675, -1.241599515311558: 11996, -1.2695951228009852: 7928, -7.43396200478643: 11})\nCounter({-0.6623007762153256: 23675, -1.238010459031821: 11996, -1.6411388693162696: 7928, -7.39116859513779: 11})\nCounter({-1.3862943611198906: 43610})\nCounter({-1.3862943611198906: 43610})\nCounter({-1.3862943611198906: 43610})\nCounter({-0.8275547298335948: 23675, -1.1740214971798035: 11996, -1.3735576868644248: 7928, -7.492388829892848: 11})\nCounter({-0.42148253543982656: 23675, -1.5822680070821724: 11996, -1.9825917156899182: 7928, -7.255591274253665: 11})\n",
      "TIME_OF_DAY\nCounter({-2.6791440212643525: 2886, -2.7129025011893066: 2825, -2.7840436012145475: 2679, -2.7959642721768585: 2605, -2.8814626522367166: 2489, -2.9094262108653637: 2435, -3.019774268034229: 2151, -3.0754631738948177: 2084, -3.133503033362572: 1885, -3.241876694840028: 1855, -3.177403212237812: 1851, -3.187207212334433: 1799, -3.2650356246020578: 1792, -3.226388517397228: 1751, -3.3119106908353904: 1587, -3.3776403973431144: 1553, -3.344812127029779: 1522, -3.3800270333208453: 1480, -3.4467184078195174: 1416, -3.49512550360656: 1296, -3.5731595062190156: 1152, -3.678520021876842: 994, -3.9052933412416304: 835, -4.04142551556621: 688})\nCounter({-3.7232808808312687: 7112, -2.9348235204669986: 6273, -3.0301337002713233: 5726, -2.847812143477369: 4771, -2.7677694358038325: 4756, -3.1354942159291497: 4504, -3.253277251585533: 4305, -3.9464244321454784: 3647, -3.5409593240373143: 2516})\nCounter({-3.330579814448202: 7046, -3.0204248861443626: 6527, -2.925114706340038: 5504, -3.163525729785036: 3677, -3.588408923750302: 3550, -3.377099830083095: 3331, -2.6839526495231496: 2435, -2.9558863650067915: 2151, -3.4258899942525267: 1855, -3.2435684374585723: 1587, -2.8381033293504077: 1553, -3.202746442938317: 1416, -2.866274206317104: 1296, -3.531250509910353: 994, -4.023726995008147: 688})\nCounter({-3.213019292755488: 3550, -2.785892687866144: 2886, -2.7485051557945237: 2825, -2.9072535448704113: 2679, -2.9682142446600985: 2605, -2.7717080528741875: 2489, -3.0640960163633815: 2435, -3.0960476161699835: 2151, -2.9512166682915275: 2084, -2.9625162235454607: 1885, -3.1425676318048765: 1855, -3.2654657681280304: 1851, -3.003101503660539: 1792, -3.3046864812813115: 1587, -3.2888131321250214: 1553, -3.242651090361859: 1522, -3.2203453328475606: 1480, -3.405612399871272: 1416, -3.5480327399130407: 1296, -3.7141596107870147: 1152, -3.527830032595521: 994, -4.125667033351141: 835, -4.3043588220945175: 688})\nCounter({-3.029260557949002: 3647, -2.7569634043367004: 2886, -2.720803422922257: 2825, -2.7152116096564787: 2679, -2.8107076803433912: 2605, -2.8005346526303407: 2489, -2.81892127888078: 2435, -3.009108742511694: 2151, -2.974791649587565: 2084, -3.1889396379931965: 1885, -3.130952380342847: 1851, -3.1366021128849666: 1799, -3.125334388038624: 1751, -3.2860424684120413: 1587, -3.3026265964275767: 1553, -3.404648210819888: 1522, -3.4816092519560167: 1480, -3.5781201523368606: 1416, -3.6849501759740466: 1296, -3.9404751367912962: 1152, -4.0830892439901385: 994, -4.1827886042132265: 835, -4.438721978350427: 688})\nCounter({-3.1780538303479458: 43610})\nCounter({-3.1780538303479458: 43610})\nCounter({-3.1780538303479458: 43610})\nCounter({-2.670453836429534: 2886, -2.7453848538972068: 2825, -2.7396377116416386: 2679, -2.7775984738808615: 2605, -2.762827156560549: 2489, -2.774626703491704: 2435, -2.86136835694632: 2151, -2.907734277504246: 2084, -3.1393885184863644: 1885, -2.924828710863546: 1855, -2.9743087681269156: 1851, -3.1566303249208705: 1799, -3.143671180278365: 1792, -3.2381233591720533: 1751, -3.37977387623508: 1587, -3.2523753818792547: 1553, -3.470745654440807: 1522, -3.5512845169248193: 1480, -3.7348972294823937: 1416, -3.849777505480816: 1296, -4.128801515189482: 1152, -4.277221520307755: 994, -4.435826550484394: 835, -4.6057255872797915: 688})\nCounter({-3.714268688118756: 4281, -3.377796451497543: 3902, -3.556044682903862: 3637, -3.3377911168838437: 3367, -3.1107336662484975: 2886, -3.175272187386069: 2825, -2.9388834093218383: 2605, -3.050109044432063: 2435, -3.6320305898817837: 1885, -3.419469147898111: 1855, -3.4409753531190743: 1851, -3.4629542598378498: 1799, -3.3575937441800234: 1587, -3.126482023216637: 1522, -2.8875901149342877: 1480, -2.36434197116974: 1416, -2.7157398580076286: 1296, -2.6446439363238983: 1152, -3.4854271156899084: 994, -3.006936872566854: 835})\n",
      "SPEED_LIMIT\nCounter({-1.0240277551858747: 15569, -1.3493455749887404: 11665, -1.9119462052941834: 6238, -2.3705611875759103: 4168, -3.0020594709046624: 2214, -3.559834202781557: 1237, -3.5041870240671655: 1129, -4.956193350517588: 613, -4.3134271942499955: 579, -5.655137648761859: 155, -6.973990729086218: 39, -10.109484945015367: 4})\nCounter({-1.5040773967762742: 15569, -1.3179751171424134: 11665, -1.8225311278948086: 6238, -2.030170492673053: 4168, -2.2925347571405443: 2214, -3.9019726695746444: 1237, -3.4965075614664802: 1129, -3.6788291182604347: 579, -5.288267030694535: 489, -4.59511985013459: 322})\nCounter({-1.1186129553747792: 15569, -1.1973938332278935: 11665, -2.144200109027999: 6238, -2.1164205449209232: 4168, -3.9219733362813143: 2366, -2.941144083269588: 2214, -4.327438444389479: 579, -4.797442073635215: 322, -5.308267697401205: 291, -5.020585624949423: 155, -5.713732805509369: 39, -6.406879986069314: 4})\nCounter({-1.0741782633308277: 15569, -1.2728394848858477: 11665, -1.8669552848396143: 6238, -2.4884464764931242: 4168, -2.9318565161654124: 2214, -3.619494040914388: 1237, -3.987218821039705: 1129, -4.218330542003092: 579, -4.872257009409756: 322, -4.603993022815077: 291, -5.085831109707815: 155, -6.184443398375925: 39, -7.437206366871293: 3, -8.130353547431238: 1})\nCounter({-1.144481485919917: 15569, -1.2899657658147772: 11665, -1.777937464576616: 6238, -2.2829491184771866: 4168, -2.983369058366212: 2214, -3.5423767587182877: 1237, -3.521323349520455: 1129, -4.6452534461728545: 579, -4.843079189502775: 322, -5.195299783092127: 291, -6.111590514966282: 155, -7.392524360428346: 39, -9.001962272862446: 4})\nCounter({-2.70805020110221: 43610})\nCounter({-2.70805020110221: 43610})\nCounter({-2.70805020110221: 43610})\nCounter({-0.984668029318892: 15569, -1.254804353634086: 11665, -2.3469357382181126: 6238, -2.2248553173491827: 4168, -2.843649517791422: 2214, -3.4870970297990946: 1237, -4.358935999102416: 1129, -4.344547261650316: 579, -4.604058457135401: 322, -5.159055299214529: 291, -5.260837993524471: 155, -6.80128303447162: 39, -7.89989532313973: 3, -8.593042503699674: 1})\nCounter({-0.6459266395023592: 15569, -1.4432466871244747: 11665, -2.6886186389734537: 6238, -2.7974214988222528: 4168, -3.525659999193468: 2214, -3.625743457750451: 1237, -5.317419468421523: 1129, -3.4346882209877414: 579, -5.653891705042736: 322, -4.865434344678466: 291, -5.877035256356946: 155, -6.570182436916891: 39, -7.2633296174768365: 4})\n",
      "ILLUMINATION\nCounter({-0.6056610051811366: 24118, -1.157289975274858: 13676, -2.3899888111923073: 3719, -3.7306924684474527: 1014, -3.935332548197103: 844, -5.338434027633375: 239})\nCounter({-0.9110136747733114: 24118, -0.8979415932059586: 13676, -2.1507045617013265: 3719, -3.4499875458315876: 1858, -4.548599834499697: 239})\nCounter({-0.7972057337907629: 24118, -0.8906589028478751: 13676, -2.384583928160131: 3719, -3.7528597837773434: 1014, -4.089332020398556: 844, -5.0056227522727115: 239})\nCounter({-0.6129000923291011: 24118, -1.1383645868432117: 13676, -2.4441200854790903: 3719, -3.783894430964088: 1014, -3.837240411669381: 844, -4.992205636888622: 239})\nCounter({-0.532010216634268: 24118, -1.248517983807167: 13676, -2.5488041926722325: 3719, -3.7227384878789413: 1014, -4.0310398475334575: 844, -5.1941906573391385: 239})\nCounter({-1.791759469228055: 43610})\nCounter({-1.791759469228055: 43610})\nCounter({-1.791759469228055: 43610})\nCounter({-0.5037323018115059: 24118, -1.2821602238977263: 13676, -2.6942187219537486: 3719, -3.837782398484124: 1858, -4.877800522886181: 239})\nCounter({-1.001252665338706: 24118, -0.6598570052054213: 13676, -2.593563612980006: 3719, -4.121508491162923: 1014, -4.165960253733757: 844, -4.6920533496305366: 239})\n",
      "ROAD_CONDITION\nCounter({-0.4625732344111501: 29495, -1.714689874031898: 6913, -1.9608807955276653: 5258, -3.0040717871794476: 1944})\nCounter({-0.22716231090912753: 29495, -2.286669637688146: 6913, -2.928523523860541: 5258, -3.0338840395183673: 1944})\nCounter({-0.2508343514593957: 29495, -2.1838687861546635: 6913, -2.6996819514316934: 5258, -3.169685580677429: 1944})\nCounter({-0.289554824465291: 29495, -2.108515970850141: 6913, -2.384105997536893: 5258, -3.275078921426758: 1944})\nCounter({-0.3172211412915822: 29495, -2.0214612325030807: 6913, -2.2937441709691435: 5258, -3.257603319762408: 1944})\nCounter({-1.3862943611198906: 43610})\nCounter({-1.3862943611198906: 43610})\nCounter({-1.3862943611198906: 43610})\nCounter({-0.3075069923984463: 29495, -1.980305073843198: 6913, -2.434022132975402: 5258, -3.2438935878434885: 1944})\nCounter({-0.32119406432510683: 29495, -2.0406555166446796: 6913, -2.205735267004128: 5258, -3.3637709761430385: 1944})\n",
      "INTERSECT_TYPE\nCounter({-0.42606658920162344: 27487, -1.8336504109372151: 7276, -1.9827632967345163: 6660, -3.8670582000600597: 940, -4.470926796181479: 506, -4.658243011949525: 384, -5.153454407913964: 280, -6.612773904048745: 54, -7.912056888179006: 18, -9.41613428495528: 5})\nCounter({-0.2790835671965491: 27487, -2.372318431008721: 13936, -4.56954300834494: 940, -5.262690188904886: 863, -3.876395827784995: 384})\nCounter({-0.45579555940850663: 27487, -1.7934247485471162: 7276, -1.955943678044891: 6660, -3.833645577073671: 940, -4.319153392855371: 506, -4.789157022101107: 384, -4.606835465307152: 280, -6.398594934535208: 77})\nCounter({-0.4873157008646667: 27487, -1.7521531942270117: 7276, -1.8244313397036576: 6660, -3.785074720271955: 940, -4.602519617509477: 506, -4.632372580659158: 384, -5.0843577044022155: 280, -6.519442229691538: 54, -7.4357329615656935: 18, -8.128880142125638: 5})\nCounter({-0.4903723521640719: 27487, -1.775864770984098: 7276, -1.7942273875586368: 6660, -3.9199418787819296: 940, -4.273958425054052: 506, -4.738666366725077: 384, -4.874211858721301: 280, -6.603450970968022: 54, -8.308199063206446: 23})\nCounter({-2.3025850929940455: 43610})\nCounter({-2.3025850929940455: 43610})\nCounter({-2.3025850929940455: 43610})\nCounter({-0.5944519909133963: 27487, -1.6606433123340436: 7276, -1.5981821427103073: 6660, -3.622301818357496: 940, -4.549063850098946: 506, -4.903235663819561: 384, -4.807925484015236: 280, -6.646204968878183: 54, -6.982677205499396: 18, -7.205820756813607: 5})\nCounter({-0.4629958920883308: 27487, -1.7303905228517629: 7276, -1.9868200517994394: 6660, -3.676300671907076: 940, -4.315380631196746: 506, -5.6503816979290855: 402, -4.861924337564815: 280, -6.161207321695077: 54, -7.259819610363186: 5})\n",
      "LANE_CLOSED\nCounter({-0.5231412857710867: 23675, -1.3181594812251252: 11996, -1.969596713574681: 7928, -8.49959932301519: 11})\nCounter({-1.9352717508502575: 23675, -2.286669637688146: 11996, -0.28946619424528236: 7928, -5.231108616854587: 11})\nCounter({-1.1956045546554195: 23675, -1.5522794985941517: 11996, -0.7256009254096838: 7928, -6.38856140554563: 11})\nCounter({-0.8450355272529108: 23675, -1.241599515311558: 11996, -1.2695951228009852: 7928, -7.43396200478643: 11})\nCounter({-0.6623007762153256: 23675, -1.238010459031821: 11996, -1.6411388693162696: 7928, -7.39116859513779: 11})\nCounter({-1.3862943611198906: 43610})\nCounter({-1.3862943611198906: 43610})\nCounter({-1.3862943611198906: 43610})\nCounter({-0.8275547298335948: 23675, -1.1740214971798035: 11996, -1.3735576868644248: 7928, -7.492388829892848: 11})\nCounter({-0.42148253543982656: 23675, -1.5822680070821724: 11996, -1.9825917156899182: 7928, -7.255591274253665: 11})\n",
      "TIME_OF_DAY\nCounter({-2.6791440212643525: 2886, -2.7129025011893066: 2825, -2.7840436012145475: 2679, -2.7959642721768585: 2605, -2.8814626522367166: 2489, -2.9094262108653637: 2435, -3.019774268034229: 2151, -3.0754631738948177: 2084, -3.133503033362572: 1885, -3.241876694840028: 1855, -3.177403212237812: 1851, -3.187207212334433: 1799, -3.2650356246020578: 1792, -3.226388517397228: 1751, -3.3119106908353904: 1587, -3.3776403973431144: 1553, -3.344812127029779: 1522, -3.3800270333208453: 1480, -3.4467184078195174: 1416, -3.49512550360656: 1296, -3.5731595062190156: 1152, -3.678520021876842: 994, -3.9052933412416304: 835, -4.04142551556621: 688})\nCounter({-3.7232808808312687: 7112, -2.9348235204669986: 6273, -3.0301337002713233: 5726, -2.847812143477369: 4771, -2.7677694358038325: 4756, -3.1354942159291497: 4504, -3.253277251585533: 4305, -3.9464244321454784: 3647, -3.5409593240373143: 2516})\nCounter({-3.330579814448202: 7046, -3.0204248861443626: 6527, -2.925114706340038: 5504, -3.163525729785036: 3677, -3.588408923750302: 3550, -3.377099830083095: 3331, -2.6839526495231496: 2435, -2.9558863650067915: 2151, -3.4258899942525267: 1855, -3.2435684374585723: 1587, -2.8381033293504077: 1553, -3.202746442938317: 1416, -2.866274206317104: 1296, -3.531250509910353: 994, -4.023726995008147: 688})\nCounter({-3.213019292755488: 3550, -2.785892687866144: 2886, -2.7485051557945237: 2825, -2.9072535448704113: 2679, -2.9682142446600985: 2605, -2.7717080528741875: 2489, -3.0640960163633815: 2435, -3.0960476161699835: 2151, -2.9512166682915275: 2084, -2.9625162235454607: 1885, -3.1425676318048765: 1855, -3.2654657681280304: 1851, -3.003101503660539: 1792, -3.3046864812813115: 1587, -3.2888131321250214: 1553, -3.242651090361859: 1522, -3.2203453328475606: 1480, -3.405612399871272: 1416, -3.5480327399130407: 1296, -3.7141596107870147: 1152, -3.527830032595521: 994, -4.125667033351141: 835, -4.3043588220945175: 688})\nCounter({-3.029260557949002: 3647, -2.7569634043367004: 2886, -2.720803422922257: 2825, -2.7152116096564787: 2679, -2.8107076803433912: 2605, -2.8005346526303407: 2489, -2.81892127888078: 2435, -3.009108742511694: 2151, -2.974791649587565: 2084, -3.1889396379931965: 1885, -3.130952380342847: 1851, -3.1366021128849666: 1799, -3.125334388038624: 1751, -3.2860424684120413: 1587, -3.3026265964275767: 1553, -3.404648210819888: 1522, -3.4816092519560167: 1480, -3.5781201523368606: 1416, -3.6849501759740466: 1296, -3.9404751367912962: 1152, -4.0830892439901385: 994, -4.1827886042132265: 835, -4.438721978350427: 688})\nCounter({-3.1780538303479458: 43610})\nCounter({-3.1780538303479458: 43610})\nCounter({-3.1780538303479458: 43610})\nCounter({-2.670453836429534: 2886, -2.7453848538972068: 2825, -2.7396377116416386: 2679, -2.7775984738808615: 2605, -2.762827156560549: 2489, -2.774626703491704: 2435, -2.86136835694632: 2151, -2.907734277504246: 2084, -3.1393885184863644: 1885, -2.924828710863546: 1855, -2.9743087681269156: 1851, -3.1566303249208705: 1799, -3.143671180278365: 1792, -3.2381233591720533: 1751, -3.37977387623508: 1587, -3.2523753818792547: 1553, -3.470745654440807: 1522, -3.5512845169248193: 1480, -3.7348972294823937: 1416, -3.849777505480816: 1296, -4.128801515189482: 1152, -4.277221520307755: 994, -4.435826550484394: 835, -4.6057255872797915: 688})\nCounter({-3.714268688118756: 4281, -3.377796451497543: 3902, -3.556044682903862: 3637, -3.3377911168838437: 3367, -3.1107336662484975: 2886, -3.175272187386069: 2825, -2.9388834093218383: 2605, -3.050109044432063: 2435, -3.6320305898817837: 1885, -3.419469147898111: 1855, -3.4409753531190743: 1851, -3.4629542598378498: 1799, -3.3575937441800234: 1587, -3.126482023216637: 1522, -2.8875901149342877: 1480, -2.36434197116974: 1416, -2.7157398580076286: 1296, -2.6446439363238983: 1152, -3.4854271156899084: 994, -3.006936872566854: 835})\n",
      "SPEED_LIMIT\nCounter({-1.0240277551858747: 15569, -1.3493455749887404: 11665, -1.9119462052941834: 6238, -2.3705611875759103: 4168, -3.0020594709046624: 2214, -3.559834202781557: 1237, -3.5041870240671655: 1129, -4.956193350517588: 613, -4.3134271942499955: 579, -5.655137648761859: 155, -6.973990729086218: 39, -10.109484945015367: 4})\nCounter({-1.5040773967762742: 15569, -1.3179751171424134: 11665, -1.8225311278948086: 6238, -2.030170492673053: 4168, -2.2925347571405443: 2214, -3.9019726695746444: 1237, -3.4965075614664802: 1129, -3.6788291182604347: 579, -5.288267030694535: 489, -4.59511985013459: 322})\nCounter({-1.1186129553747792: 15569, -1.1973938332278935: 11665, -2.144200109027999: 6238, -2.1164205449209232: 4168, -3.9219733362813143: 2366, -2.941144083269588: 2214, -4.327438444389479: 579, -4.797442073635215: 322, -5.308267697401205: 291, -5.020585624949423: 155, -5.713732805509369: 39, -6.406879986069314: 4})\nCounter({-1.0741782633308277: 15569, -1.2728394848858477: 11665, -1.8669552848396143: 6238, -2.4884464764931242: 4168, -2.9318565161654124: 2214, -3.619494040914388: 1237, -3.987218821039705: 1129, -4.218330542003092: 579, -4.872257009409756: 322, -4.603993022815077: 291, -5.085831109707815: 155, -6.184443398375925: 39, -7.437206366871293: 3, -8.130353547431238: 1})\nCounter({-1.144481485919917: 15569, -1.2899657658147772: 11665, -1.777937464576616: 6238, -2.2829491184771866: 4168, -2.983369058366212: 2214, -3.5423767587182877: 1237, -3.521323349520455: 1129, -4.6452534461728545: 579, -4.843079189502775: 322, -5.195299783092127: 291, -6.111590514966282: 155, -7.392524360428346: 39, -9.001962272862446: 4})\nCounter({-2.70805020110221: 43610})\nCounter({-2.70805020110221: 43610})\nCounter({-2.70805020110221: 43610})\nCounter({-0.984668029318892: 15569, -1.254804353634086: 11665, -2.3469357382181126: 6238, -2.2248553173491827: 4168, -2.843649517791422: 2214, -3.4870970297990946: 1237, -4.358935999102416: 1129, -4.344547261650316: 579, -4.604058457135401: 322, -5.159055299214529: 291, -5.260837993524471: 155, -6.80128303447162: 39, -7.89989532313973: 3, -8.593042503699674: 1})\nCounter({-0.6459266395023592: 15569, -1.4432466871244747: 11665, -2.6886186389734537: 6238, -2.7974214988222528: 4168, -3.525659999193468: 2214, -3.625743457750451: 1237, -5.317419468421523: 1129, -3.4346882209877414: 579, -5.653891705042736: 322, -4.865434344678466: 291, -5.877035256356946: 155, -6.570182436916891: 39, -7.2633296174768365: 4})\n",
      "ILLUMINATION\nCounter({-0.6056610051811366: 24118, -1.157289975274858: 13676, -2.3899888111923073: 3719, -3.7306924684474527: 1014, -3.935332548197103: 844, -5.338434027633375: 239})\nCounter({-0.9110136747733114: 24118, -0.8979415932059586: 13676, -2.1507045617013265: 3719, -3.4499875458315876: 1858, -4.548599834499697: 239})\nCounter({-0.7972057337907629: 24118, -0.8906589028478751: 13676, -2.384583928160131: 3719, -3.7528597837773434: 1014, -4.089332020398556: 844, -5.0056227522727115: 239})\nCounter({-0.6129000923291011: 24118, -1.1383645868432117: 13676, -2.4441200854790903: 3719, -3.783894430964088: 1014, -3.837240411669381: 844, -4.992205636888622: 239})\nCounter({-0.532010216634268: 24118, -1.248517983807167: 13676, -2.5488041926722325: 3719, -3.7227384878789413: 1014, -4.0310398475334575: 844, -5.1941906573391385: 239})\nCounter({-1.791759469228055: 43610})\nCounter({-1.791759469228055: 43610})\nCounter({-1.791759469228055: 43610})\nCounter({-0.5037323018115059: 24118, -1.2821602238977263: 13676, -2.6942187219537486: 3719, -3.837782398484124: 1858, -4.877800522886181: 239})\nCounter({-1.001252665338706: 24118, -0.6598570052054213: 13676, -2.593563612980006: 3719, -4.121508491162923: 1014, -4.165960253733757: 844, -4.6920533496305366: 239})\nCounter({0: 24560, 4: 8104, 8: 5379, 3: 3381, 9: 1412, 2: 591, 1: 183})\nCounter({0: 43492, 8: 104, 4: 7, 9: 5, 1: 2})\n### TESTING naive_bayes: PASSED 0/0\n###\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import collections\n",
    "def naive_bayes_test(naive_bayes):\n",
    "    df = static_df_data\n",
    "    cl = naive_bayes(df)\n",
    "    # cl = naive_bayes(static_df_data)\n",
    "    \n",
    "    # test.equal(cl.log_prior.tolist(), [-0.28626858222129903, -1.3905468592226538])\n",
    "    # \n",
    "    # test.true(isinstance(cl.predictor['age'], GaussianPredictor) and\n",
    "    #     isinstance(cl.predictor['work_class'], CategoricalPredictor) and\n",
    "    #     isinstance(cl.predictor['final_weight'], GaussianPredictor) and\n",
    "    #     isinstance(cl.predictor['education'], CategoricalPredictor) and\n",
    "    #     isinstance(cl.predictor['education_num'], GaussianPredictor) and\n",
    "    #     isinstance(cl.predictor['marital_status'], CategoricalPredictor) and\n",
    "    #     isinstance(cl.predictor['occupation'], CategoricalPredictor) and\n",
    "    #     isinstance(cl.predictor['relationship'], CategoricalPredictor) and\n",
    "    #     isinstance(cl.predictor['race'], CategoricalPredictor) and\n",
    "    #     isinstance(cl.predictor['sex'], CategoricalPredictor) and\n",
    "    #     isinstance(cl.predictor['capital_gain'], GaussianPredictor) and\n",
    "    #     isinstance(cl.predictor['capital_loss'], GaussianPredictor) and\n",
    "    #     isinstance(cl.predictor['hours_per_week'], GaussianPredictor) and\n",
    "    #     isinstance(cl.predictor['native_country'], CategoricalPredictor))    \n",
    "    # \n",
    "    ll = cl.log_likelihood(df.drop(\"label\", axis=\"columns\"))\n",
    "    # test.equal(tuple(ll.shape), (2, 30162))\n",
    "    # test.equal(ll[:,:2].tolist(), [[-49.84977999441486, -50.38520793711001], [-53.407383777033196, -51.30832341372758]])\n",
    "    # \n",
    "    lp = cl.predict(df.drop(\"label\", axis=\"columns\"))\n",
    "    print(collections.Counter(static_df_data['label']))\n",
    "    print(collections.Counter(lp))\n",
    "    # test.equal(tuple(lp.shape), (30162,))\n",
    "    # test.equal(sum(lp), 5407)\n",
    "    # test.equal(lp[:10].tolist(), [0]*8 + [1]*2)\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    \"\"\" Naive Bayes classifier for a mixture of continuous and categorical attributes.\n",
    "        We use GaussianPredictor for continuous attributes and CategoricalPredictor for categorical ones.\n",
    "        \n",
    "        attr:\n",
    "            predictor : Dict[column_name,model] -- model for each column\n",
    "            log_prior : np.ndarray -- the (log) prior probability of each class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, alpha=1.):\n",
    "        \"\"\"initialize predictors for each feature and compute class prior\n",
    "        \n",
    "        args:\n",
    "            df : pd.DataFrame -- processed dataframe, without any missing values.\n",
    "        \n",
    "        kwargs:\n",
    "            alpha : float -- smoothing factor for prior probability\n",
    "        \"\"\"\n",
    "        label = df[\"label\"]\n",
    "        k = max(label)+1\n",
    "        self.log_prior = np.zeros(k)\n",
    "        n = len(label)\n",
    "        for i in range(k):\n",
    "            ybools= (label==i)\n",
    "            \n",
    "            nt=np.sum( ybools )\n",
    "            self.log_prior[i] = np.log( (nt+alpha)/(n+(k*alpha)))\n",
    "        \n",
    "        self.predictor = dict()\n",
    "        types = dict(df.dtypes)\n",
    "        for key in types:\n",
    "            if key != \"label\" and key!= \"index\":\n",
    "                if str(types[key])==\"int64\":\n",
    "                    self.predictor[key]=GaussianPredictor(k).fit(df[key],label)\n",
    "                elif str(types[key])==\"object\":\n",
    "                    self.predictor[key]=CategoricalPredictor(k).fit(df[key],label)\n",
    "                else:\n",
    "                    raise TypeError\n",
    "        self.k=k            \n",
    "        pass\n",
    "\n",
    "    def log_likelihood(self, x):\n",
    "        \"\"\"log_likelihood for input instances from log_prior and partial_log_likelihood of feature predictors\n",
    "\n",
    "        args:\n",
    "            x : pd.DataFrame -- processed dataframe (ignore label if present)\n",
    "\n",
    "        returns : np.ndarray[num_classes, len(x)] -- array of log-likelihood\n",
    "        \"\"\"\n",
    "        # try:\n",
    "        #     x = x.drop(\"label\")\n",
    "        # except:\n",
    "        #     pass\n",
    "        \n",
    "        # like = np.zeros((self.k, len(x)))\n",
    "        like = np.array(([self.log_prior,]*len(x))).transpose()\n",
    "        # print (self.k,len(x))\n",
    "        # print(like.shape)\n",
    "        # for i in range(self.k):\n",
    "        #     for j in range(len(x)):\n",
    "        #         \n",
    "        #         # like[i][j] = self.log_prior[i]+np.sum( self.predictor[key].partial_log_likelihood(x[j])[i][j] for key in self.predictor )\n",
    "        #         like[i][j]=self.log_prior[i]\n",
    "        for key in self.predictor:\n",
    "            model = self.predictor[key]\n",
    "            z= model.partial_log_likelihood(x[key])\n",
    "            like +=z\n",
    "            print(key)\n",
    "            for i in range(z.shape[0]):\n",
    "                print(collections.Counter(z[i]))\n",
    "            # like[i][j] += model.partial_log_likelihood(x[key])[i][j]\n",
    "                             \n",
    "        return like           \n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"predicts label for input instances, breaks ties in favor of the class with lower id.\n",
    "\n",
    "        args:\n",
    "            x : pd.DataFrame -- processed dataframe (ignore label if present)\n",
    "\n",
    "        returns : np.ndarray[len(x)] -- vector of class labels\n",
    "        \"\"\"\n",
    "        pred = np.argmax(self.log_likelihood(x),axis = 0)\n",
    "        return pred\n",
    "\n",
    "@test\n",
    "def naive_bayes(*args, **kwargs):\n",
    "    return NaiveBayesClassifier(*args, **kwargs)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}