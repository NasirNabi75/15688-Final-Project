{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carcrash Severity Prediction\n",
    "\n",
    "Downloading the datasets:\n",
    "\n",
    "Please run `wget https://data.wprdc.org/dataset/3130f583-9499-472b-bb5a-f63a6ff6059a/resource/ec578660-2d3f-489d-9ba1-af0ebfc3b140/download/all-crashes-2004-2018.csv.zip`\n",
    "or visit this link to get the Allegheny County crash data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import ckanapi\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.metrics import f1_score\n",
    "site = \"https://data.wprdc.org\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_resource_data(site,resource_id,count=50):\n",
    "    # Use the datastore_search API endpoint to get <count> records from\n",
    "    # a CKAN resource.\n",
    "    ckan = ckanapi.RemoteCKAN(site)\n",
    "    response = ckan.action.datastore_search(id=resource_id, limit=count)\n",
    "\n",
    "    # A typical response is a dictionary like this\n",
    "    #{u'_links': {u'next': u'/api/action/datastore_search?offset=3',\n",
    "    #             u'start': u'/api/action/datastore_search'},\n",
    "    # u'fields': [{u'id': u'_id', u'type': u'int4'},\n",
    "    #             {u'id': u'pin', u'type': u'text'},\n",
    "    #             {u'id': u'number', u'type': u'int4'},\n",
    "    #             {u'id': u'total_amount', u'type': u'float8'}],\n",
    "    # u'limit': 3,\n",
    "    # u'records': [{u'_id': 1,\n",
    "    #               u'number': 11,\n",
    "    #               u'pin': u'0001B00010000000',\n",
    "    #               u'total_amount': 13585.47},\n",
    "    #              {u'_id': 2,\n",
    "    #               u'number': 2,\n",
    "    #               u'pin': u'0001C00058000000',\n",
    "    #               u'total_amount': 7827.64},\n",
    "    #              {u'_id': 3,\n",
    "    #               u'number': 1,\n",
    "    #               u'pin': u'0001C01661006700',\n",
    "    #               u'total_amount': 3233.59}],\n",
    "    # u'resource_id': u'd1e80180-5b2e-4dab-8ec3-be621628649e',\n",
    "    # u'total': 88232}\n",
    "    data = response['records']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "crash_data_2017 = get_resource_data(site,resource_id=\"bf8b3c7e-8d60-40df-9134-21606a451c1a\",count=999999999) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "   UNBELTED  WORK_ZONE URBAN_RURAL  DRIVER_20YR  MC_DRINKING_DRIVER  \\\n0         0          0           3            0                   0   \n1         0          0           3            0                   0   \n2         0          0           3            0                   0   \n3         0          0           3            0                   0   \n4         0          0           3            0                   0   \n\n   CELL_PHONE  PROPERTY_DAMAGE_ONLY ACCESS_CTRL  HIT_GDRAIL LOCATION_TYPE  \\\n0           0                     0        None           0            00   \n1           0                     1        None           0            00   \n2           0                     0        None           0            00   \n3           0                     1        None           0            00   \n4           0                     0        None           0            00   \n\n   ...  CROSS_MEDIAN  WZ_LN_CLOSURE  LOCAL_ROAD  DRUG_RELATED  PEDESTRIAN  \\\n0  ...             0           None           0             0           0   \n1  ...             0           None           1             0           0   \n2  ...             0           None           0             0           0   \n3  ...             0           None           1             0           0   \n4  ...             0           None           1             0           0   \n\n   MODERATE_INJURY TAILGATING  ROAD_CONDITION      LATITUDE _id  \n0                0          0               1  40 28:01.602   1  \n1                0          0               1  40 26:56.157   2  \n2                0          0               1  40 26:54.669   3  \n3                0          0               1  40 25:30.597   4  \n4                0          0               0  40 27:31.670   5  \n\n[5 rows x 191 columns]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "len(crash_data_2017)\n",
    "df = pd.DataFrame(crash_data_2017)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Add a graph/animation to show the car crash data\n",
    "maybe the geospatial data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "c:\\python37\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (79,183,189) have mixed types. Specify dtype option on import or set low_memory=False.\n  interactivity=interactivity, compiler=compiler, result=result)\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "    TIME_OF_DAY ILLUMINATION WEATHER ROAD_CONDITION INTERSECT_TYPE  \\\n29           11            1       1              5              0   \n48           13            1       4              2              0   \n52           10            1       2              2              0   \n53           15            1       1              2              0   \n54           16            1       1              3              0   \n59           13            1       2              2              0   \n61           11            1       4              5              0   \n67           21            2       4              4              2   \n69            7            1       4              5              0   \n70           10            1       4              3              0   \n\n   SCH_BUS_IND  PERSON_COUNT  AUTOMOBILE_COUNT  MOTORCYCLE_COUNT  \\\n29           0             2                 2                 0   \n48           0             3                 2                 0   \n52           0             4                 2                 0   \n53           0             3                 2                 0   \n54           0             1                 1                 0   \n59           0             2                 2                 0   \n61           0             3                 1                 0   \n67           0             1                 1                 0   \n69           0             2                 2                 0   \n70           0             2                 1                 0   \n\n    HEAVY_TRUCK_COUNT  FATAL_COUNT  INJURY_COUNT  MAX_SEVERITY_LEVEL  \\\n29                  0            0             0                   0   \n48                  0            0             0                   0   \n52                  0            0             0                   0   \n53                  0            0             2                   4   \n54                  0            0             1                   4   \n59                  0            0             1                   4   \n61                  0            0             0                   0   \n67                  0            0             0                   0   \n69                  0            0             2                   3   \n70                  0            0             0                   0   \n\n   LANE_CLOSED HAZARDOUS_TRUCK  MAJOR_INJURY  SPEED_LIMIT  \n29           0               0             0           40  \n48           1               0             0           35  \n52           0               0             0           40  \n53           2               0             0           25  \n54           2               0             0           45  \n59           2               0             0           40  \n61           2               0             0           35  \n67           0               0             0           45  \n69           1               0             0           35  \n70           0               0             0           35  \n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 43610 entries, 1 to 180963\nData columns (total 12 columns):\nROAD_CONDITION       43610 non-null object\nINTERSECT_TYPE       43610 non-null object\nLANE_CLOSED          43610 non-null object\nTIME_OF_DAY          43610 non-null int64\nSPEED_LIMIT          43610 non-null int64\nILLUMINATION         43610 non-null object\nMOTORCYCLE_COUNT     43610 non-null int64\nHEAVY_TRUCK_COUNT    43610 non-null int64\nHAZARDOUS_TRUCK      43610 non-null object\nAUTOMOBILE_COUNT     43610 non-null int64\nSCH_BUS_IND          43610 non-null object\nWEATHER              43610 non-null object\ndtypes: int64(5), object(7)\nmemory usage: 4.3+ MB\nNone\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 43610 entries, 1 to 180963\nData columns (total 5 columns):\nPERSON_COUNT          43610 non-null int64\nFATAL_COUNT           43610 non-null int64\nINJURY_COUNT          43610 non-null int64\nMAX_SEVERITY_LEVEL    43610 non-null int64\nMAJOR_INJURY          43610 non-null int64\ndtypes: int64(5)\nmemory usage: 2.0 MB\nNone\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import wget\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "#clean sch_bus_ind y/n to integer 0,1\n",
    "\n",
    "def type_boolean(c):\n",
    "    if c == \"Y\": return 1\n",
    "    elif c == \"N\": return 0\n",
    "    # elif c == \"nan\": return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "    # raise ValueError(c)\n",
    "\n",
    "def ROAD_CONDITION(c): # 8 is other 9 is unknown, 1,7->2, 3->4, 4->3, 5,6->5, 2,8,9->nan\n",
    "    if c == 1 or c == 7:\n",
    "        return 2\n",
    "    elif c == 3:\n",
    "        return 4\n",
    "    elif c == 4:\n",
    "        return 3\n",
    "    elif c == 5 or c == 6:\n",
    "        return 5\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def INTERSECT_TYPE(c): # 10 is other 99 is unkonw\n",
    "    if c <= 9:\n",
    "        return c\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def ILLUMINATION(c):\n",
    "    if c <= 6:\n",
    "        return c\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def WEATHER(c):\n",
    "    if c <= 7:\n",
    "        return c\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def TIME(c): # extract only the hour\n",
    "    if c <= 2500:\n",
    "        return c // 100\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "if not Path('all-crashes-2004-2018.csv.zip').exists():\n",
    "    wget.download(\"https://data.wprdc.org/dataset/3130f583-9499-472b-bb5a-f63a6ff6059a/resource/ec578660-2d3f-489d-9ba1-af0ebfc3b140/download/all-crashes-2004-2018.csv.zip\")\n",
    "# zf = zipfile.ZipFile('all-crashes-2004-2018.csv.zip') \n",
    "df_io = pd.read_csv('all-crashes-2004-2018.csv.zip')\n",
    "# print(df.head())\n",
    "# print(list(df))\n",
    "# static_columns = \"ROAD_CONDITION,INTERSECT_TYPE,URBAN_RURAL,DISTRICT,STATE_ROAD,LOCAL_ROAD,SNOW_SLUSH_ROAD,LANE_CLOSED,TIME_OF_DAY,SPEED_LIMIT\"\n",
    "# dynamic_columns = \"ILLUMINATION,MOTORCYCLE_COUNT,HEAVY_TRUCK_COUNT,WEATHER,HAZARDOUS_TRUCK,SCH_BUS_IND,AUTOMOBILE_COUNT\"\n",
    "# output_columns = \"PERSON_COUNT,FATAL_COUNT,INJURY_COUNT,MAX_SEVERITY_LEVEL,MAJOR_INJURY\"\n",
    "# df_io = df[(static_columns+\",\"+dynamic_columns+\",\"+output_columns).split(',')]\n",
    "# print(df_io.head())\n",
    "# print(df_io.dtypes)\n",
    "# print(df_io.info())\n",
    "\n",
    "# df_io['SCH_BUS_IND'] = df_io['SCH_BUS_IND'].apply(type_boolean)\n",
    "\n",
    "\n",
    "\n",
    "# df_io['ROAD_CONDITION'] = df_io['ROAD_CONDITION'].apply(ROAD_CONDITION)\n",
    "# df_io['INTERSECT_TYPE'] = df_io['INTERSECT_TYPE'].apply(INTERSECT_TYPE)\n",
    "# df_io['ILLUMINATION'] = df_io['ILLUMINATION'].apply(ILLUMINATION)\n",
    "# df_io['WEATHER'] = df_io['WEATHER'].apply(WEATHER)\n",
    "# df_io['TIME_OF_DAY'] = df_io['TIME_OF_DAY'].apply(TIME)\n",
    "\n",
    "# df_io = df_io.astype(\"Int64\")\n",
    "\n",
    "# print(df_io.head())\n",
    "# print(df_io.dtypes)\n",
    "# print(df_io.info())\n",
    "\n",
    "# drop col that will not be used\n",
    "static = ['ROAD_CONDITION', 'INTERSECT_TYPE', 'LANE_CLOSED', 'TIME_OF_DAY', 'SPEED_LIMIT', 'ILLUMINATION']\n",
    "dynamic = ['MOTORCYCLE_COUNT', 'HEAVY_TRUCK_COUNT', 'HAZARDOUS_TRUCK', 'AUTOMOBILE_COUNT', 'SCH_BUS_IND', 'WEATHER']\n",
    "label = ['PERSON_COUNT', 'FATAL_COUNT', 'INJURY_COUNT', 'MAX_SEVERITY_LEVEL', 'MAJOR_INJURY']\n",
    "categorical = ['ROAD_CONDITION', 'INTERSECT_TYPE', 'LANE_CLOSED', 'ILLUMINATION', 'HAZARDOUS_TRUCK', 'SCH_BUS_IND', 'WEATHER']\n",
    "gussian = ['TIME_OF_DAY', 'SPEED_LIMIT', 'MOTORCYCLE_COUNT', 'HEAVY_TRUCK_COUNT', 'AUTOMOBILE_COUNT']\n",
    "data = static + dynamic\n",
    "for col in df_io.columns:\n",
    "    if col not in static and col not in dynamic and col not in label:\n",
    "        df_io.drop(col, axis = 1, inplace = True)  \n",
    "# print(df_io[15:25])\n",
    "# df_io['TIME_OF_DAY'] = df_io['TIME_OF_DAY'].astype(\"Int64\")\n",
    "\n",
    "\n",
    "# clean data\n",
    "df_io['SCH_BUS_IND'] = df_io['SCH_BUS_IND'].apply(type_boolean)\n",
    "df_io['ROAD_CONDITION'] = df_io['ROAD_CONDITION'].apply(ROAD_CONDITION)\n",
    "df_io['INTERSECT_TYPE'] = df_io['INTERSECT_TYPE'].apply(INTERSECT_TYPE)\n",
    "df_io['ILLUMINATION'] = df_io['ILLUMINATION'].apply(ILLUMINATION)\n",
    "df_io['WEATHER'] = df_io['WEATHER'].apply(WEATHER)\n",
    "df_io['TIME_OF_DAY'] = df_io['TIME_OF_DAY'].apply(TIME)\n",
    "\n",
    "# drop rows contain nan\n",
    "df_io = df_io.dropna()\n",
    "df_io = df_io.astype(\"int64\")\n",
    "df_io[categorical] = df_io[categorical].astype(\"object\")\n",
    "print(df_io[15:25])\n",
    "# group data into dataset/label\n",
    "df_data = df_io[data].copy()\n",
    "df_label = df_io[label].copy()\n",
    "print(df_data.info())\n",
    "print(df_label.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# split the dataset with 33% as train data\n",
    "from sklearn.model_selection import train_test_split\n",
    "comb_df_data = df_io[data].copy()\n",
    "static_df_data = df_io[static].copy()\n",
    "dynamic_df_data = df_io[dynamic].copy()\n",
    "\n",
    "# static\n",
    "smj_X_train, smj_X_test, smj_y_train, smj_y_test = train_test_split(static_df_data, df_io['MAJOR_INJURY'], test_size=0.33, random_state=42)\n",
    "sseverity_X_train, sseverity_X_test, sseverity_y_train, sseverity_y_test = train_test_split(static_df_data, df_io['MAX_SEVERITY_LEVEL'], test_size=0.33, random_state=42)\n",
    "\n",
    "# dynamic\n",
    "dmj_X_train, dmj_X_test, dmj_y_train, dmj_y_test = train_test_split(dynamic_df_data, df_io['MAJOR_INJURY'], test_size=0.33, random_state=42)\n",
    "dseverity_X_train, dseverity_X_test, dseverity_y_train, dseverity_y_test = train_test_split(dynamic_df_data, df_io['MAX_SEVERITY_LEVEL'], test_size=0.33, random_state=42)\n",
    "\n",
    "# combined\n",
    "cmj_X_train, cmj_X_test, cmj_y_train, cmj_y_test = train_test_split(df_data, df_io['MAJOR_INJURY'], test_size=0.33, random_state=42)\n",
    "cseverity_X_train, cseverity_X_test, cseverity_y_train, cseverity_y_test = train_test_split(df_data, df_io['MAX_SEVERITY_LEVEL'], test_size=0.33, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class GaussianPredictor:\n",
    "    \n",
    "    \"\"\" Feature predictor for a normally distributed real-valued, continuous feature.\n",
    "\n",
    "        attr:\n",
    "            k : int -- number of classes\n",
    "            mu : np.ndarray[k] -- vector containing per class mean of the feature\n",
    "            sigma : np.ndarray[k] -- vector containing per class std. deviation of the feature\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k):\n",
    "        \"\"\" constructor\n",
    "\n",
    "        args : k -- number of classes\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.mu = np.zeros(k)\n",
    "        self.sigma = np.zeros(k)\n",
    "        pass\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        \"\"\"update predictor statistics (mu, sigma) for Gaussian distribution\n",
    "\n",
    "        args:\n",
    "            x : pd.Series -- feature values\n",
    "            y : np.Series -- class labels\n",
    "            \n",
    "        return : GaussianPredictor -- return self for convenience\n",
    "        \n",
    "        \"\"\"\n",
    "        # df = pd.DataFrame({\"values\":x,\"labels\":y})\n",
    "        # groups = df.groupby(\"labels\")\n",
    "        # self.mu = np.array((groups.mean()))\n",
    "        # self.sigma = np.sqrt(np.array(groups.var(ddof=0)))\n",
    "        y=np.array(y)\n",
    "        x=np.array(x)\n",
    "        \n",
    "        \n",
    "        for i in range(self.k):\n",
    "            ybools = (y==i)\n",
    "            # given_i = np.array([x[j] for j in range(len(y)) if y[j] == i])\n",
    "            given_i = np.extract(ybools,x)\n",
    "            self.mu[i]=given_i.mean()\n",
    "            self.sigma[i]=np.sqrt(given_i.var())\n",
    "        return self\n",
    "            \n",
    "    def partial_log_likelihood(self, x):\n",
    "        \"\"\" log likelihood of feature values x according to each class\n",
    "\n",
    "        args:\n",
    "            x : pd.Series -- feature values\n",
    "\n",
    "        return: np.ndarray[self.k, len(x)] : log likelihood for this feature for each class\n",
    "        \"\"\"\n",
    "        logpdfs = np.zeros((self.k,len(x)))\n",
    "        for i in range(self.k):\n",
    "            logpdfs[i]=stats.norm(loc=self.mu[i],scale=self.sigma[i]).logpdf(x)\n",
    "        return logpdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class CategoricalPredictor:\n",
    "    \"\"\" Feature predictor for a categorical feature.\n",
    "\n",
    "        attr: \n",
    "            k : int -- number of classes\n",
    "            p : Dict[feature_value, np.ndarray[k]] -- dictionary of vectors containing per-class probability of a feature value;\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k):\n",
    "        \"\"\" constructor\n",
    "\n",
    "        args : k -- number of classes\n",
    "        \"\"\"\n",
    "        self.k=k\n",
    "\n",
    "    def fit(self, x, y, alpha=1.):\n",
    "        \"\"\" initializes the predictor statistics (p) for Categorical distribution\n",
    "        \n",
    "        args:\n",
    "            x : pd.Series -- feature values\n",
    "            y : pd.Series -- class labels\n",
    "        \n",
    "        kwargs:\n",
    "            alpha : float -- smoothing factor\n",
    "\n",
    "        return : CategoricalPredictor -- returns self for convenience:\n",
    "        \"\"\"\n",
    "        y=np.array(y)\n",
    "        x=np.array(x)\n",
    "        self.p = {}\n",
    "        for char in set(x):\n",
    "#             print(self.k)\n",
    "#             print(char)\n",
    "#             print(self.p)\n",
    "            self.p[char]=np.zeros(self.k)\n",
    "            \n",
    "        \n",
    "        for i in range(self.k):\n",
    "            # n=sum([1 for j in range(len(y)) if y[j] == i])\n",
    "            ybools= (y==i)\n",
    "            \n",
    "            n=np.sum( ybools )\n",
    "            for char in set(x):\n",
    "                xbools= (x == char)\n",
    "                # nj=sum([1 for j in range(len(y)) if x[j] == char and y[j] == i ])\n",
    "                nj = np.sum(np.logical_and(xbools,ybools))\n",
    "                (self.p[char])[i] = (nj + alpha)/ (n+len(set(x))*alpha)\n",
    "        return self\n",
    "\n",
    "    def partial_log_likelihood(self, x):\n",
    "        \"\"\" log likelihood of feature values x according to each class\n",
    "\n",
    "        args:\n",
    "            x : pd.Series -- vector of feature values\n",
    "\n",
    "        return : np.ndarray[self.k, len(x)] -- matrix of log likelihood for this feature\n",
    "        \"\"\"\n",
    "        like = np.zeros((self.k,len(x)))\n",
    "        \n",
    "        for i in range(self.k):\n",
    "            for j,char in enumerate(x):\n",
    "                like[i][j]=np.log(self.p[char][i])\n",
    "        return like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "# not wokring properly\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, df_feature, df_label, alpha=1.):\n",
    "        self.predictor = {}\n",
    "        self.l = df_label.value_counts()\n",
    "        self.unique_label = len(df_label.unique())\n",
    "        self.log_prior = np.zeros(self.unique_label)\n",
    "        n = len(df_label)\n",
    "        print(n, self.unique_label)\n",
    "        # Initialize log_prior\n",
    "        for i in range(self.unique_label):\n",
    "            ybools= (label==i)\n",
    "            nt=np.sum( ybools )\n",
    "            self.log_prior[i] = np.log((nt+alpha)/(n+(self.unique_label*alpha)))\n",
    "            \n",
    "        # Initialize predictor\n",
    "        for col in df_feature.columns:\n",
    "            if df_feature[col].dtype == 'object':\n",
    "#                 print(df_feature[col].shape)\n",
    "#                 print(df_feature[col].values.reshape(-1,1))\n",
    "#                 self.predictor[col] = CategoricalPredictor(self.unique_label).fit(df_feature[col], df_label)\n",
    "                self.predictor[col] = MultinomialNB().fit(df_feature[col].values.reshape(-1,1), df_label)\n",
    "            elif df_feature[col].dtype == 'int64':\n",
    "#                 self.predictor[col] = GaussianPredictor(self.unique_label).fit(df_feature[col], df_label)\n",
    "                self.predictor[col] = GaussianNB().fit(df_feature[col].values.reshape(-1,1), df_label)\n",
    "        \n",
    "    def log_likelihood(self, df_feature):\n",
    "        likelihood = np.array(([self.log_prior,]*len(df_feature)))\n",
    "        for col in self.predictor:\n",
    "            model = self.predictor[col]\n",
    "#             print(df_feature[col].values.reshape(-1,1))\n",
    "#             print(model.predict_log_proba(df_feature[col].values.reshape(-1,1)))\n",
    "#             print(collections.Counter(df_feature[col].values.reshape(-1,1)))\n",
    "#             unique, counts = np.unique(df_feature[col].values.reshape(-1,1), return_counts=True)\n",
    "#             p = dict(zip(unique, counts))\n",
    "#             print (p)\n",
    "\n",
    "            z = model.predict_log_proba(df_feature[col].values.reshape(-1,1))\n",
    "            likelihood += z\n",
    "#             likelihood += model.partial_log_likelihood(df_feature[col])\n",
    "#             print(col)\n",
    "#             for i in range(z.shape[0]):\n",
    "#                 print(collections.Counter(z[i]))\n",
    "#         print(likelihood.shape)\n",
    "        return likelihood \n",
    "        \n",
    "    def predict(self, df_feature):\n",
    "        pred = np.argmax(self.log_likelihood(df_feature),axis = 1)\n",
    "        return pred\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# working \n",
    "class NaiveBayesClassifier3:\n",
    "    \"\"\" Naive Bayes classifier for a mixture of continuous and categorical attributes.\n",
    "        We use GaussianPredictor for continuous attributes and CategoricalPredictor for categorical ones.\n",
    "        \n",
    "        attr:\n",
    "            predictor : Dict[column_name,model] -- model for each column\n",
    "            log_prior : np.ndarray -- the (log) prior probability of each class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, label, alpha=1.):\n",
    "        \"\"\"initialize predictors for each feature and compute class prior\n",
    "        \n",
    "        args:\n",
    "            df : pd.DataFrame -- processed dataframe, without any missing values.\n",
    "        \n",
    "        kwargs:\n",
    "            alpha : float -- smoothing factor for prior probability\n",
    "        \"\"\"\n",
    "        # label = df[\"label\"]\n",
    "        self.predictor = {}\n",
    "        self.l = label.value_counts()\n",
    "        self.unique_label = len(label.unique())\n",
    "        print(self.unique_label)\n",
    "        self.log_prior = np.zeros(self.unique_label)\n",
    "        n = len(label.unique())\n",
    "        print(n)\n",
    "        # Initialize log_prior\n",
    "        for i in range(self.unique_label):\n",
    "            ybools= (label==i)\n",
    "            nt=np.sum( ybools )\n",
    "            self.log_prior[i] = np.log( (nt+alpha)/(n+(self.unique_label*alpha)))\n",
    "        types = dict(df.dtypes)\n",
    "        for key in types:\n",
    "            if key != \"label\" and key!= \"index\":\n",
    "                if str(types[key])==\"int64\":\n",
    "                    self.predictor[key]=GaussianPredictor(n).fit(df[key],label)\n",
    "                elif str(types[key])==\"object\":\n",
    "                    self.predictor[key]=CategoricalPredictor(n).fit(df[key],label)\n",
    "                else:\n",
    "                    raise TypeError\n",
    "                                \n",
    "\n",
    "    def log_likelihood(self, x):\n",
    "        \"\"\"log_likelihood for input instances from log_prior and partial_log_likelihood of feature predictors\n",
    "\n",
    "        args:\n",
    "            x : pd.DataFrame -- processed dataframe (ignore label if present)\n",
    "\n",
    "        returns : np.ndarray[num_classes, len(x)] -- array of log-likelihood\n",
    "        \"\"\"\n",
    "\n",
    "        like = np.array(([self.log_prior,]*len(x))).transpose()\n",
    "\n",
    "        for key in self.predictor:\n",
    "            model = self.predictor[key]\n",
    "            z= model.partial_log_likelihood(x[key])\n",
    "            like +=z\n",
    "            print(key)\n",
    "            for i in range(z.shape[0]):\n",
    "                print(collections.Counter(z[i]))\n",
    "                             \n",
    "        return like           \n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"predicts label for input instances, breaks ties in favor of the class with lower id.\n",
    "\n",
    "        args:\n",
    "            x : pd.DataFrame -- processed dataframe (ignore label if present)\n",
    "\n",
    "        returns : np.ndarray[len(x)] -- vector of class labels\n",
    "        \"\"\"\n",
    "        pred = np.argmax(self.log_likelihood(x),axis = 0)\n",
    "        return pred\n",
    "# working\n",
    "class NaiveBayesClassifier2:\n",
    "    \"\"\" Naive Bayes classifier for a mixture of continuous and categorical attributes.\n",
    "        We use GaussianPredictor for continuous attributes and CategoricalPredictor for categorical ones.\n",
    "        \n",
    "        attr:\n",
    "            predictor : Dict[column_name,model] -- model for each column\n",
    "            log_prior : np.ndarray -- the (log) prior probability of each class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, label, alpha=1.):\n",
    "        \"\"\"initialize predictors for each feature and compute class prior\n",
    "        \n",
    "        args:\n",
    "            df : pd.DataFrame -- processed dataframe, without any missing values.\n",
    "        \n",
    "        kwargs:\n",
    "            alpha : float -- smoothing factor for prior probability\n",
    "        \"\"\"\n",
    "        # label = df[\"label\"]\n",
    "        k = max(label)+1\n",
    "        self.log_prior = np.zeros(k)\n",
    "        n = len(label)\n",
    "        for i in range(k):\n",
    "            ybools= (label==i)          \n",
    "            nt=np.sum( ybools )\n",
    "            self.log_prior[i] = np.log( (nt+alpha)/(n+(k*alpha)))\n",
    "        \n",
    "        self.predictor = dict()\n",
    "        types = dict(df.dtypes)\n",
    "        for key in types:\n",
    "            if key != \"label\" and key!= \"index\":\n",
    "                if str(types[key])==\"int64\":\n",
    "                    self.predictor[key]=GaussianPredictor(k).fit(df[key],label)\n",
    "                elif str(types[key])==\"object\":\n",
    "                    self.predictor[key]=CategoricalPredictor(k).fit(df[key],label)\n",
    "                else:\n",
    "                    raise TypeError\n",
    "                    \n",
    "        self.k=k            \n",
    "        pass\n",
    "\n",
    "    def log_likelihood(self, x):\n",
    "        \"\"\"log_likelihood for input instances from log_prior and partial_log_likelihood of feature predictors\n",
    "\n",
    "        args:\n",
    "            x : pd.DataFrame -- processed dataframe (ignore label if present)\n",
    "\n",
    "        returns : np.ndarray[num_classes, len(x)] -- array of log-likelihood\n",
    "        \"\"\"\n",
    "        # try:\n",
    "        #     x = x.drop(\"label\")\n",
    "        # except:\n",
    "        #     pass\n",
    "        \n",
    "        # like = np.zeros((self.k, len(x)))\n",
    "        like = np.array(([self.log_prior,]*len(x))).transpose()\n",
    "        # print (self.k,len(x))\n",
    "        # print(like.shape)\n",
    "        # for i in range(self.k):\n",
    "        #     for j in range(len(x)):\n",
    "        #         \n",
    "        #         # like[i][j] = self.log_prior[i]+np.sum( self.predictor[key].partial_log_likelihood(x[j])[i][j] for key in self.predictor )\n",
    "        #         like[i][j]=self.log_prior[i]\n",
    "        for key in self.predictor:\n",
    "            model = self.predictor[key]\n",
    "            z= model.partial_log_likelihood(x[key])\n",
    "            like +=z\n",
    "#             print(key)\n",
    "#             for i in range(z.shape[0]):\n",
    "#                 print(collections.Counter(z[i]))\n",
    "            # like[i][j] += model.partial_log_likelihood(x[key])[i][j]\n",
    "                             \n",
    "        return like           \n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"predicts label for input instances, breaks ties in favor of the class with lower id.\n",
    "\n",
    "        args:\n",
    "            x : pd.DataFrame -- processed dataframe (ignore label if present)\n",
    "\n",
    "        returns : np.ndarray[len(x)] -- vector of class labels\n",
    "        \"\"\"\n",
    "        pred = np.argmax(self.log_likelihood(x),axis = 0)\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "def label_normalization(label,label_test):\n",
    "    # print(label)\n",
    "    labellist = pd.Series(np.append(label,label_test)).unique()\n",
    "#     print(labellist)\n",
    "    labellist.sort()\n",
    "    revdic = dict()\n",
    "    for i,z in enumerate(labellist):\n",
    "        revdic[z] = i\n",
    "#     print (revdic)\n",
    "    f = (lambda s: revdic[s])\n",
    "    return np.array(list(map(f, label))),np.array(list(map(f, label_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def cal_accuracy(y_pred, y_test):\n",
    "    correct_counter = 0\n",
    "    total = len(y_test)\n",
    "    if total != len(y_pred):\n",
    "        print(\"param size not match.\")\n",
    "    for idx in range(total):\n",
    "        if (int)(y_pred[idx]) == (int)(y_test[idx]):\n",
    "            correct_counter += 1\n",
    "    print(correct_counter, total)\n",
    "    return correct_counter / total\n",
    "\n",
    "def f1(pred, ground):\n",
    "    \"\"\" evaluates a classifier based on a supplied validation data\n",
    "\n",
    "    args:\n",
    "        pred: numpy.ndarray(bool) -- predictions\n",
    "        ground: numpy.ndarray(bool) -- known ground-truth values\n",
    "    \n",
    "    return : double -- the F1 score of the predictions\n",
    "    \"\"\"\n",
    "    pred = np.array(pred, dtype=bool)\n",
    "    ground = np.array(ground, dtype=bool)\n",
    "    realT = sum(ground)\n",
    "    predT = sum(pred)\n",
    "    bothT = float(sum([1 for i in range(ground.shape[0]) if pred[i] and ground[i]]))\n",
    "    precision = bothT/predT\n",
    "    recall = bothT/realT\n",
    "    print(\"precision and recal in f1:\",precision, recall)\n",
    "    return 2*(precision*recall)/(precision+recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Static Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "8118 14392\nAccuracy:  0.5640633685380767\n",
      "14209 14392\nAccuracy:  0.9872846025569761\nCounter({0: 14392})\nCounter({0: 14209, 1: 183})\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Normalize the labels\n",
    "ss_train,ss_test = label_normalization(sseverity_y_train,sseverity_y_test)\n",
    "smj_train, smj_test = label_normalization(smj_y_train,smj_y_test)\n",
    "\n",
    "# Severity level\n",
    "ss_classifier = NaiveBayesClassifier2(sseverity_X_train, pd.Series(ss_train))\n",
    "ss_predict = ss_classifier.predict(sseverity_X_test)\n",
    "c = collections.Counter(ss_predict)\n",
    "d = collections.Counter(ss_test)\n",
    "print(\"Accuracy: \", cal_accuracy(ss_predict, ss_test))\n",
    "\n",
    "# Major injury\n",
    "smj_classifier = NaiveBayesClassifier2(smj_X_train, pd.Series(smj_y_train))\n",
    "smj_predict = smj_classifier.predict(smj_X_test)\n",
    "c = collections.Counter(smj_predict)\n",
    "d = collections.Counter(smj_test)\n",
    "print(\"Accuracy: \", cal_accuracy(smj_predict, smj_test))\n",
    "\n",
    "\n",
    "\n",
    "# d = collections.Counter(sseverity_y_test)\n",
    "print(c)\n",
    "print(d)\n",
    "# print(sseverity_y_test)\n",
    "# print(label_normalization(sseverity_y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Counter({0: 13845, 1: 532, 4: 15})\nCounter({0: 8121, 4: 2625, 5: 1778, 3: 1155, 6: 478, 2: 172, 1: 63})\n7871 14392\nAccuracy:  0.5469010561423013\n",
      "Counter({0: 14315, 1: 77})\nCounter({0: 14209, 1: 183})\n14136 14392\nAccuracy:  0.9822123401889938\nprecision and recal in f1: 0.025974025974025976 0.01092896174863388\nF1:  0.015384615384615385\nF1':  0.015384615384615385\n14209 14392\nAccuracy:  0.9872846025569761\nprecision and recal in f1: nan 0.0\nF1:  nan\n",
      "F1':  0.0\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "c:\\python37\\lib\\site-packages\\ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in true_divide\nc:\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n  'recall', 'true', average, warn_for)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Normalize the labels\n",
    "ds_train, ds_test = label_normalization(dseverity_y_train,dseverity_y_test)\n",
    "dmj_train, dmj_test = label_normalization(dmj_y_train,dmj_y_test)\n",
    "\n",
    "# Severity level\n",
    "ds_classifier = NaiveBayesClassifier2(dseverity_X_train, pd.Series(ds_train))\n",
    "ds_predict = ds_classifier.predict(dseverity_X_test)\n",
    "c = collections.Counter(ds_predict)\n",
    "d = collections.Counter(ds_test)\n",
    "print(c)\n",
    "print(d)\n",
    "print(\"Accuracy: \", cal_accuracy(ds_predict, ds_test))\n",
    "\n",
    "# Major injury\n",
    "dmj_classifier = NaiveBayesClassifier2(dmj_X_train, pd.Series(dmj_y_train))\n",
    "dmj_predict = dmj_classifier.predict(dmj_X_test)\n",
    "c = collections.Counter(dmj_predict)\n",
    "d = collections.Counter(dmj_test)\n",
    "print(c)\n",
    "print(d)\n",
    "print(\"Accuracy: \", cal_accuracy(dmj_predict, dmj_test))\n",
    "print(\"F1: \",f1(dmj_predict,dmj_test))\n",
    "print(\"F1': \",f1_score(dmj_predict,dmj_test))\n",
    "\n",
    "dumj_predict = np.zeros(dmj_test.shape)\n",
    "print(\"predict all zeros Accuracy: \", cal_accuracy(dumj_predict, dmj_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Counter({0: 13817, 1: 526, 4: 46, 3: 2, 5: 1})\nCounter({0: 8121, 4: 2625, 5: 1778, 3: 1155, 6: 478, 2: 172, 1: 63})\n7867 14392\nAccuracy:  0.5466231239577543\n",
      "Counter({0: 14257, 1: 135})\nCounter({0: 14209, 1: 183})\n14082 14392\nAccuracy:  0.9784602556976097\nprecision and recal in f1: 0.02962962962962963 0.02185792349726776\nF1:  0.02515723270440252\n14209 14392\nAccuracy:  0.9872846025569761\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Normalize the labels\n",
    "cs_train, cs_test = label_normalization(cseverity_y_train, cseverity_y_test)\n",
    "cmj_train, cmj_test = label_normalization(cmj_y_train, cmj_y_test)\n",
    "\n",
    "# Severity level\n",
    "cs_classifier = NaiveBayesClassifier2(cseverity_X_train, pd.Series(cs_train))\n",
    "cs_predict = cs_classifier.predict(cseverity_X_test)\n",
    "c = collections.Counter(cs_predict)\n",
    "d = collections.Counter(cs_test)\n",
    "print(c)\n",
    "print(d)\n",
    "print(\"Accuracy: \", cal_accuracy(cs_predict, cs_test))\n",
    "\n",
    "# Major injury\n",
    "cmj_classifier = NaiveBayesClassifier2(cmj_X_train, pd.Series(cmj_y_train))\n",
    "cmj_predict = cmj_classifier.predict(cmj_X_test)\n",
    "c = collections.Counter(cmj_predict)\n",
    "d = collections.Counter(cmj_test)\n",
    "print(c)\n",
    "print(d)\n",
    "print(\"Accuracy: \", cal_accuracy(cmj_predict, cmj_test))\n",
    "print(\"F1: \",f1(cmj_predict,cmj_test))\n",
    "\n",
    "dumj_predict = np.zeros(dmj_test.shape)\n",
    "print(\"predict all 0s Accuracy: \", cal_accuracy(dumj_predict, cmj_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-Time Severity Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}